{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following are the two important types of deep neural networks:\n",
    "1- Convolutional Neural Networks\n",
    "2- Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A convolutional neural network uses three basic ideas:\n",
    "1- Local respective fields\n",
    "2- Convolution\n",
    "3- Pooling\n",
    "\n",
    "\"\"\"CNN utilizes spatial correlations that exist within the input data. Each concurrent layer of\n",
    "a neural network connects some input neurons. This specific region is called local receptive\n",
    "field. Local receptive field focusses on the hidden neurons. The hidden neurons process\n",
    "the input data inside the mentioned field not realizing the changes outside the specific\n",
    "boundary.\"\"\"\n",
    "# https://i.ibb.co/1J9qkcJ/cnn-1.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Implementation of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Include the necessary modules for TensorFlow and the data set modules, which\n",
    "# are needed to compute the CNN model.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step 2: Declare a function called run_cnn(), which includes various parameters and\n",
    "optimization variables with declaration of data placeholders. These optimization variables\n",
    "will declare the training pattern.\n",
    "\"\"\"\n",
    "def run_cnn():\n",
    " mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    " learning_rate = 0.0001\n",
    " epochs = 10\n",
    " batch_size = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step 3: In this step, we will declare the training data placeholders with input parameters\n",
    "- for 28 x 28 pixels = 784. This is the flattened image data that is drawn from\n",
    "mnist.train.nextbatch().\n",
    "We can reshape the tensor according to our requirements. The first value (-1) tells\n",
    "function to dynamically shape that dimension based on the amount of data passed to it.\n",
    "The two middle dimensions are set to the image size (i.e. 28 x 28).\"\"\"\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "x_shaped = tf.reshape(x, [-1, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_new_conv_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f31ae16f9b4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \"\"\"Step 4: Now it is important to create some convolutional layers:\n\u001b[0;32m      2\u001b[0m \"\"\"\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlayer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_new_conv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_shaped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'layer1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlayer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_new_conv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'layer2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_new_conv_layer' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"Step 4: Now it is important to create some convolutional layers:\n",
    "\"\"\"\n",
    "layer1 = create_new_conv_layer(x_shaped, 1, 32, [5, 5], [2, 2], name='layer1')\n",
    "layer2 = create_new_conv_layer(layer1, 32, 64, [5, 5], [2, 2], name='layer2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-39f450b14ded>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m weights and bias values for this layer, then activate with ReLU.\"\"\"\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mflattened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m7\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mwd1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m7\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.03\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wd1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mbd1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bd1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'layer2' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"Step 5: Let us flatten the output ready for the fully connected output stage - after two\n",
    "layers of stride 2 pooling with the dimensions of 28 x 28, to dimension of 14 x 14 or\n",
    "minimum 7 x 7 x,y co-ordinates, but with 64 output channels. To create the fully connected\n",
    "with \"dense\" layer, the new shape needs to be [-1, 7 x 7 x 64]. We can set up some\n",
    "weights and bias values for this layer, then activate with ReLU.\"\"\"\n",
    "\n",
    "flattened = tf.reshape(layer2, [-1, 7 * 7 * 64])\n",
    "wd1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1000], stddev=0.03),name='wd1')\n",
    "bd1 = tf.Variable(tf.truncated_normal([1000], stddev=0.01), name='bd1')\n",
    "dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
    "dense_layer1 = tf.nn.relu(dense_layer1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step 6: Another layer with specific softmax activations with the required optimizer defines\n",
    "the accuracy assessment, which makes the setup of initialization operator.\"\"\"\n",
    "wd2 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.03), name='wd2')\n",
    "bd2 = tf.Variable(tf.truncated_normal([10], stddev=0.01), name='bd2')\n",
    "dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
    "y_ = tf.nn.softmax(dense_layer2)\n",
    "cross_entropy =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=dense_layer2,labels=y))\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-13-04ed44113f37>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-04ed44113f37>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    with tf.Session() as sess:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Step 7: We should set up recording variables. This adds up a summary to store the\n",
    "accuracy of data. \"\"\"\n",
    "\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('E:\\TensorFlowProject')\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "        for epoch in range(epochs):\n",
    "         avg_cost = 0\n",
    "         for i in range(total_batch):\n",
    "             batch_x, batch_y =mnist.train.next_batch(batch_size=batch_size)\n",
    "             _, c = sess.run([optimiser, cross_entropy], feed_dict={x:batch_x, y: batch_y})\n",
    "             avg_cost += c / total_batch\n",
    "         test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y:mnist.test.labels})\n",
    "         summary = sess.run(merged, feed_dict={x: mnist.test.images, y:mnist.test.labels})\n",
    "         writer.add_summary(summary, epoch)\n",
    "        print(\"\\nTraining complete!\")\n",
    "    \n",
    "    writer.add_graph(sess.graph)\n",
    "    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y:mnist.test.labels}))\n",
    "\n",
    "def create_new_conv_layer(input_data, num_input_channels, num_filters,filter_shape, pool_shape, name):\n",
    "     conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels,num_filters]\n",
    "     weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03),name=name+'_W')\n",
    "     bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "    #Out layer defines the output\n",
    "     out_layer =tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
    "     out_layer += bias\n",
    "     out_layer = tf.nn.relu(out_layer)\n",
    "     ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "     strides = [1, 2, 2, 1]\n",
    "     out_layer = tf.nn.max_pool(out_layer, ksize=ksize, strides=strides,padding='SAME')\n",
    "     return out_layer\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    run_cnn()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
