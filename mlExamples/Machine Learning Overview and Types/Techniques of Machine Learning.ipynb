{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 Different Types of Learning in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Problems**\n",
    "\n",
    "    1. Supervised Learning\n",
    "    2. Unsupervised Learning\n",
    "    3. Reinforcement Learning\n",
    "    \n",
    "    \n",
    "**Hybrid Learning Problems**\n",
    "\n",
    "    4. Semi-Supervised Learning\n",
    "    5. Self-Supervised Learning\n",
    "    6. Multi-Instance Learning\n",
    "    \n",
    "**Statistical Inference**\n",
    "\n",
    "    7. Inductive Learning\n",
    "    8. Deductive Inference\n",
    "    9. Transductive Learning\n",
    "    \n",
    "**Learning Techniques**\n",
    "\n",
    "    10. Multi-Task Learning\n",
    "    11. Active Learning\n",
    "    12. Online Learning\n",
    "    13. Transfer Learning\n",
    "    14. Ensemble Learning    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-  Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning is the types of machine learning in which machines are trained using well \"labelled\" training data, and on basis of that data, machines predict the output. The labelled data means some input data is already tagged with the correct output.\n",
    "\n",
    "**Example** :  Risk Assessment, Image classification, Fraud Detection, spam filtering, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://static.javatpoint.com/tutorial/machine-learning/images/supervised-machine-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Of Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://static.javatpoint.com/tutorial/machine-learning/images/supervised-machine-learning-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Regression algorithms are used if there is a relationship between the input variable and the output variable. It is used for the prediction of continuous variables, such as Weather forecasting, Market Trends, etc\n",
    "\n",
    "\n",
    "    1- Linear Regression\n",
    "    2- Regression Trees\n",
    "    3- Non-Linear Regression\n",
    "    4- Bayesian Linear Regression\n",
    "    5- Polynomial Regression\n",
    "    \n",
    "## Classification\n",
    "\n",
    "Classification algorithms are used when the output variable is categorical, which means there are two classes such as Yes-No, Male-Female, True-false, etc.\n",
    "\n",
    "    1- Random Forest\n",
    "    2- Decision Trees\n",
    "    3- Logistic Regression\n",
    "    4- Support vector Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-  Unsupervised Learning\n",
    "\n",
    "Unsupervised Learning is a machine learning technique in which the users do not need to supervise the model. Instead, it allows the model to work on its own to discover patterns and information that was previously undetected. It mainly deals with the unlabelled data.\n",
    "\n",
    "\n",
    "\n",
    "Clustering is an important concept when it comes to unsupervised learning. It mainly deals with finding a structure or pattern in a collection of uncategorized data. \n",
    "\n",
    "ou can also modify how many clusters your algorithms should identify.\n",
    "\n",
    "#### Clustering\n",
    "\n",
    "![title](https://www.guru99.com/images/1/030819_1030_Unsupervise3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Types\n",
    "\n",
    "    1- Hierarchical clustering\n",
    "    2- K-means clustering\n",
    "    3- K-NN (k nearest neighbors)\n",
    "    4- Principal Component Analysis\n",
    "    5- Singular Value Decomposition\n",
    "    6- Independent Component Analysis\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is a machine learning training method based on rewarding desired behaviors and/or punishing undesired ones. In general, a reinforcement learning agent is able to perceive and interpret its environment, take actions and learn through trial and error.\n",
    "\n",
    "\n",
    "In Reinforcement Learning (RL), agents are trained on a reward and punishment mechanism. The agent is rewarded for correct moves and punished for the wrong ones. In doing so, the agent tries to minimize wrong moves and maximize the right ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRuLj4b73AR9Jewu_TZ3z27RqHnlvN2Mjc0AA&usqp=CAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Semi-Supervised Learning\n",
    "\n",
    "Semi-supervised machine learning is a combination of supervised and unsupervised machine learning methods.\n",
    "\n",
    "\n",
    "Semi-supervised learning is the type of machine learning that uses a combination of a small amount of labeled data and a large amount of unlabeled data to train models. This approach to machine learning is a combination of supervised machine learning, which uses labeled training data, and unsupervised learning, which uses unlabeled training data. \n",
    "\n",
    "![title](https://algorithmia.com/blog/wp-content/uploads/2020/07/image3-2-1536x585.jpg)\n",
    "\n",
    "Why is Semi-Supervised Machine Learning Important?\n",
    "\n",
    "When you don’t have enough labeled data to produce an accurate model and you don’t have the ability or resources to get more data, you can use semi-supervised techniques to increase the size of your training data. For example, imagine you are developing a model intended to detect fraud for a large bank. Some fraud you know about, but other instances of fraud are slipping by without your knowledge. You can label the dataset with the fraud instances you’re aware of, but the rest of your data will remain unlabelled:\n",
    "\n",
    "![title](https://www.datarobot.com/wp-content/uploads/2018/03/Screen-Shot-2018-03-22-at-11.53.41-AM.png)\n",
    "\n",
    "\n",
    "You can use a semi-supervised learning algorithm to label the data, and retrain the model with the newly labeled dataset:\n",
    "\n",
    "![title](https://www.datarobot.com/wp-content/uploads/2018/03/Screen-Shot-2018-03-22-at-11.56.45-AM-1024x647.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Self-Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-supervised learning (SSL) is a method of machine learning. It learns from unlabeled sample data. It can be regarded as an intermediate form between supervised and unsupervised learning. It is based on an artificial neural network. ... Second, the actual task is performed with supervised or unsupervised learning.\n",
    "\n",
    "![title](https://miro.medium.com/max/700/1*lavhAToKGO9Bo4j3U7n4TA.jpeg)\n",
    "\n",
    "\n",
    "Self-supervised Learning is an unsupervised learning method where the supervised learning task is created out of the unlabelled input data.\n",
    "\n",
    "This task could be as simple as given the upper-half of the image, predict the lower-half of the same image, or given the grayscale version of the colored image, predict the RGB channels of the same image, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Self-supervised Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning requires usually a lot of labelled data. Getting good quality labelled data is an expensive and time-consuming task specially for a complex task such as object detection, instance segmentation where more detailed annotations are desired. On the other hand, \n",
    "\n",
    "the unlabelled data is readily available in abundance. The motivation behind Self-supervised learning is to learn useful representations of the data from unlabelled pool of data using self-supervision first and then fine-tune the representations with few labels for the supervised downstream task. The downstream task could be as simple as image classification or complex task such as semantic segmentation, object detection, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lately, in natural language processing, Transformer models have achieved a lot of success. Transformers like Bert[1], T5[2], etc. applied the idea of self-supervision to NLP tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They first train the model with large unlabelled data and then fine-tuning the model with few labelled data examples. Similar self-supervised learning methods have been researched for computer vision as well \n",
    "\n",
    "**Many self-supervised learning methods have been researched but contrastive learning methods seem to be work better than others for computer vision, hence in this post, I would concentrate on contrastive learning-based self-supervised learning methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Contrastive Learning?\n",
    "\n",
    "\n",
    "suppose we have a function f(represented by any deep network Resnet50 for example), given an input x, it gives us the features f(x) as output.\n",
    "Contrastive Learning states that for any positive pairs x1 and x2, the respective outputs f(x1) and f(x2) should be similar to each other and for a negative input x3, f(x1) and f(x2) both should be dissimilar to f(x3).\n",
    "\n",
    "![title](https://miro.medium.com/max/700/1*fdAU4VJtnclv0rGrfzUu4g.png)\n",
    "\n",
    "\n",
    "The positive pair could be two crops of same image(lets say top-left and bottom right), two frames of same video file, two augmented views(horizontally flipped version for instance) of same image, etc. and respective negatives could be a crop from different image, frame from different video, augmented view of different image, etc.\n",
    "\n",
    "[More about Contrastive learning](https://towardsdatascience.com/self-supervised-learning-methods-for-computer-vision-c25ec10a91bd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications of self-supervised learning\n",
    "\n",
    "\n",
    "1- Natural Language Processing (NLP)\n",
    "\n",
    "Self-supervised learning helps predict the missing words within a text in. This is achieved by showing segments of texts to a giant neural network with billions of parameters, i.e., the likes of **OpenAI’s GPT-3 and Google’s BERT**.\n",
    "You mask 15% of the text to force the network to predict the pieces of words that are missing.\n",
    "\n",
    "2- Computer vision\n",
    "\n",
    "SimCLR is a framework used to learn visual representations in images using self-supervised learning. The framework performs two main tasks: a pretext task and the downstream (real) task. Self-supervised learning is used in the pretext task.\n",
    "\n",
    "It involves performing simple augmentation tasks such as random cropping, random color distortions, and random Gaussian blur on input images. This process enables the model to learn better representations of the input images. These results are passed into the downstream task module, that performs the main tasks such as detection and classification tasks.\n",
    "\n",
    "[more](https://www.section.io/engineering-education/what-is-self-supervised-learning/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Multi-Instance Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**multiple-instance learning (MIL) is a type of supervised learning. From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept***\n",
    "\n",
    "Note : Basically use in Pathology sector\n",
    "\n",
    "MIL is a variation of supervised learning that is more suitable to pathology applications. \n",
    "\n",
    "The technique involves assigning a single class label to a collection of inputs — in this context, referred to as a bag of instances. While it is assumed that labels exist for each instance within a bag, there is no access to those labels and they remain unknown during training. \n",
    "\n",
    "A bag is typically labeled as negative if all instances in the bag are negative, or positive if there is at least one positive instance (known as the standard MIL assumption). A simple example is shown in the figure below in which we only know whether a keychain contains the key that can open a given door. This allows us to infer that the green key can open the door.\n",
    "\n",
    "![title](https://miro.medium.com/max/700/1*2dHiAk7NnBNh-jC18Q1X6A.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of where MIL is applied are:\n",
    "\n",
    "    - Molecule activity\n",
    "    - Predicting binding sites of Calmodulin binding proteins[6]\n",
    "    - Predicting function for alternatively spliced isoforms Li, Menon & et al. (2014),Eksi et al. (2013)\n",
    "    - Image classification Maron & Ratan (1998)\n",
    "    - Text or document categorization Kotzias et al. (2015)\n",
    "    - Predicting functional binding sites of MicroRNA targets Bandyopadhyay, Ghosh & et al. (2015)\n",
    "    - Medical image classification Zhu et al. (2016), P.J.Sudharshan et al. (2019)\n",
    "    \n",
    "Numerous researchers have worked on adapting classical classification techniques, such as support vector machines or boosting, to work within the context of multiple-instance learning.\n",
    "\n",
    "\n",
    "[To know More](https://towardsdatascience.com/attention-based-deep-multiple-instance-learning-1bb3df857e24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble is the art of combining diverse set of learners (individual models) together to improvise on the stability and predictive power of the model. In the above example, the way we combine all the predictions together will be termed as Ensemble Learning.\n",
    "\n",
    "Ensemble modeling is a powerful way to improve the performance of your model.\n",
    "\n",
    "[More on Ensemble Learning](https://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "\n",
    "Transfer learning is nothing just fine tune of the models.\n",
    "\n",
    "\n",
    "\n",
    "[Refer this video for Transfer learning](https://www.youtube.com/watch?v=L7qjQu2ry2Q)\n",
    "\n",
    "![title](https://cdn-images-1.medium.com/max/800/1*J4-8rlPZFnHKtmyxEJeRYg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transfer learning we can use in three ways:\n",
    "    \n",
    "    - ConvNet Fixed feature extractor\n",
    "    - Fine tune the ConvNet\n",
    "    - Pretrained Models\n",
    "\n",
    "![alt text](transferlearning.png \"Title\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is overall picture of small dataset and large dataset then how and what part of model to remove. \n",
    "\n",
    "- if small dataset just remove the last \"class object\" layer and output layer, other will be fix. no change needed.\n",
    "\n",
    "- If Dataset is larger then you can just remove the all the fully connected flatten layer part and add as per your choice\n",
    "\n",
    "Check in Below image\n",
    "\n",
    "![alt text](small.png \"Title\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- ConvNet Fixed feature extractor  or train as classifier\n",
    "\n",
    "In Deep learning: Hierarcial based feature extraction\n",
    "\n",
    "![alt text](part1.png \"Title\")\n",
    "\n",
    "![alt text](part2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Fine Tune\n",
    "\n",
    "In FIne tune it only depends on how many layer you want to freeze based on your dataset choice...if less then dataset you only freeze flatten layer, if more data then you have remove some convolution block and flatten  layer \n",
    "\n",
    "![alt text](fine_tune2.png \"Title\")\n",
    "\n",
    "![alt text](fine_tune.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To more please visit in keras page](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
    "\n",
    "![title](https://blog.keras.io/img/imgclf/vgg16_original.png)\n",
    "\n",
    "![title](https://blog.keras.io/img/imgclf/vgg16_modified.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this is the entire code for \n",
    "    1- feature extractor\n",
    "    2- Fine tuning\n",
    "    3- Directly using vgg16 model\n",
    "\n",
    "\n",
    "[Here is github link to download implement](https://github.com/anujshah1003/Transfer-Learning-in-keras---custom-data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "# import time\n",
    "# from vgg16 import VGG16\n",
    "# from keras.preprocessing import image\n",
    "# from keras.applications.imagenet_utils import preprocess_input\n",
    "# from imagenet_utils import decode_predictions\n",
    "# from keras.layers import Dense, Activation, Flatten\n",
    "# from keras.layers import merge, Input\n",
    "# from keras.models import Model\n",
    "# from keras.utils import np_utils\n",
    "# from sklearn.utils import shuffle\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# img_path = 'elephant.jpg'\n",
    "# img = image.load_img(img_path, target_size=(224, 224))\n",
    "# x = image.img_to_array(img)\n",
    "# print (x.shape)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "# print (x.shape)\n",
    "# x = preprocess_input(x)\n",
    "# print('Input image shape:', x.shape)\n",
    "\n",
    "# # Loading the training data\n",
    "# PATH = os.getcwd()\n",
    "# # Define data path\n",
    "# data_path = PATH + '/data'\n",
    "# data_dir_list = os.listdir(data_path)\n",
    "\n",
    "# img_data_list=[]\n",
    "\n",
    "# for dataset in data_dir_list:\n",
    "# \timg_list=os.listdir(data_path+'/'+ dataset)\n",
    "# \tprint ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "# \tfor img in img_list:\n",
    "# \t\timg_path = data_path + '/'+ dataset + '/'+ img\n",
    "# \t\timg = image.load_img(img_path, target_size=(224, 224))\n",
    "# \t\tx = image.img_to_array(img)\n",
    "# \t\tx = np.expand_dims(x, axis=0)\n",
    "# \t\tx = preprocess_input(x)\n",
    "# #\t\tx = x/255\n",
    "# \t\tprint('Input image shape:', x.shape)\n",
    "# \t\timg_data_list.append(x)\n",
    "\n",
    "# img_data = np.array(img_data_list)\n",
    "# #img_data = img_data.astype('float32')\n",
    "# print (img_data.shape)\n",
    "# img_data=np.rollaxis(img_data,1,0)\n",
    "# print (img_data.shape)\n",
    "# img_data=img_data[0]\n",
    "# print (img_data.shape)\n",
    "\n",
    "\n",
    "# # Define the number of classes\n",
    "# num_classes = 4\n",
    "# num_of_samples = img_data.shape[0]\n",
    "# labels = np.ones((num_of_samples,),dtype='int64')\n",
    "\n",
    "# labels[0:202]=0\n",
    "# labels[202:404]=1\n",
    "# labels[404:606]=2\n",
    "# labels[606:]=3\n",
    "\n",
    "# names = ['cats','dogs','horses','humans']\n",
    "\n",
    "# # convert class labels to on-hot encoding\n",
    "# Y = np_utils.to_categorical(labels, num_classes)\n",
    "\n",
    "# #Shuffle the dataset\n",
    "# x,y = shuffle(img_data,Y, random_state=2)\n",
    "# # Split the dataset\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# #########################################################################################\n",
    "# # Custom_vgg_model_1\n",
    "# #Training the classifier alone\n",
    "# image_input = Input(shape=(224, 224, 3))\n",
    "\n",
    "# model = VGG16(input_tensor=image_input, include_top=True,weights='imagenet')\n",
    "# model.summary()\n",
    "# last_layer = model.get_layer('fc2').output\n",
    "# #x= Flatten(name='flatten')(last_layer)\n",
    "# out = Dense(num_classes, activation='softmax', name='output')(last_layer)\n",
    "# custom_vgg_model = Model(image_input, out)\n",
    "# custom_vgg_model.summary()\n",
    "\n",
    "# for layer in custom_vgg_model.layers[:-1]:\n",
    "# \tlayer.trainable = False\n",
    "\n",
    "# custom_vgg_model.layers[3].trainable\n",
    "\n",
    "# custom_vgg_model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# t=time.time()\n",
    "# #\tt = now()\n",
    "# hist = custom_vgg_model.fit(X_train, y_train, batch_size=32, epochs=12, verbose=1, validation_data=(X_test, y_test))\n",
    "# print('Training time: %s' % (t - time.time()))\n",
    "# (loss, accuracy) = custom_vgg_model.evaluate(X_test, y_test, batch_size=10, verbose=1)\n",
    "\n",
    "# print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,accuracy * 100))\n",
    "\n",
    "\n",
    "# ####################################################################################################################\n",
    "\n",
    "# #Training the feature extraction also\n",
    "\n",
    "# image_input = Input(shape=(224, 224, 3))\n",
    "\n",
    "# model = VGG16(input_tensor=image_input, include_top=True,weights='imagenet')\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# last_layer = model.get_layer('block5_pool').output\n",
    "# x= Flatten(name='flatten')(last_layer)\n",
    "# x = Dense(128, activation='relu', name='fc1')(x)\n",
    "# x = Dense(128, activation='relu', name='fc2')(x)\n",
    "# out = Dense(num_classes, activation='softmax', name='output')(x)\n",
    "# custom_vgg_model2 = Model(image_input, out)\n",
    "# custom_vgg_model2.summary()\n",
    "\n",
    "# # freeze all the layers except the dense layers\n",
    "# for layer in custom_vgg_model2.layers[:-3]:\n",
    "# \tlayer.trainable = False\n",
    "\n",
    "# custom_vgg_model2.summary()\n",
    "\n",
    "# custom_vgg_model2.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])\n",
    "\n",
    "# t=time.time()\n",
    "# #\tt = now()\n",
    "# hist = custom_vgg_model2.fit(X_train, y_train, batch_size=32, epochs=12, verbose=1, validation_data=(X_test, y_test))\n",
    "# print('Training time: %s' % (t - time.time()))\n",
    "# (loss, accuracy) = custom_vgg_model2.evaluate(X_test, y_test, batch_size=10, verbose=1)\n",
    "\n",
    "# print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,accuracy * 100))\n",
    "\n",
    "# #%%\n",
    "# import matplotlib.pyplot as plt\n",
    "# # visualizing losses and accuracy\n",
    "# train_loss=hist.history['loss']\n",
    "# val_loss=hist.history['val_loss']\n",
    "# train_acc=hist.history['acc']\n",
    "# val_acc=hist.history['val_acc']\n",
    "# xc=range(12)\n",
    "\n",
    "# plt.figure(1,figsize=(7,5))\n",
    "# plt.plot(xc,train_loss)\n",
    "# plt.plot(xc,val_loss)\n",
    "# plt.xlabel('num of Epochs')\n",
    "# plt.ylabel('loss')\n",
    "# plt.title('train_loss vs val_loss')\n",
    "# plt.grid(True)\n",
    "# plt.legend(['train','val'])\n",
    "# #print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "# plt.style.use(['classic'])\n",
    "\n",
    "# plt.figure(2,figsize=(7,5))\n",
    "# plt.plot(xc,train_acc)\n",
    "# plt.plot(xc,val_acc)\n",
    "# plt.xlabel('num of Epochs')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.title('train_acc vs val_acc')\n",
    "# plt.grid(True)\n",
    "# plt.legend(['train','val'],loc=4)\n",
    "# #print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "# plt.style.use(['classic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
