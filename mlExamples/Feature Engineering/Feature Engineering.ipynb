{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Feature engineering is about creating new input features from your existing ones\n",
    "\n",
    "In general, you can think of data cleaning as a process of subtraction and feature engineering as a process of addition\n",
    "\n",
    "Feature engineering is the science (and art) of extracting more information from existing data. You are not adding any new data here, but you are actually making the data you already have more useful.\n",
    "\n",
    "For example, let’s say you are trying to predict foot fall in a shopping mall based on dates. If you try and use the dates directly, you may not be able to extract meaningful insights from the data. This is because the foot fall is less affected by the day of the month than it is by the day of the week. Now this information about day of week is implicit in your data. You need to bring it out to make your model better.\n",
    "\n",
    "Defintion :\n",
    "\n",
    "\"This exercising of bringing out information from data in known as feature engineering\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process of Feature Engineering\n",
    "\n",
    "Feature engineering itself can be divided in 2 steps:\n",
    "\n",
    "•\tVariable transformation.\n",
    "\n",
    "•\tVariable / Feature creation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Variable Transformation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data modelling, transformation refers to the replacement of a variable by a function. For instance, replacing a variable x by the square / cube root or logarithm x is a transformation. In other words, transformation is a process that changes the distribution or relationship of a variable with others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should we use Variable Transformation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the situations where variable transformation is a requisite:\n",
    "\n",
    "•\tWhen we want to change the scale of a variable or standardize the values of a variable for better understanding. While this transformation is a must if you have data in different scales, this transformation does not change the shape of the variable distribution\n",
    "\n",
    "•\tWhen we can transform complex non-linear relationships into linear relationships. Existence of a linear relationship between variables is easier to comprehend compared to a non-linear or curved relation. Transformation helps us to convert a non-linear relation into linear relation. Scatter plot can be used to find the relationship between two continuous variables. These transformations also improve the prediction. Log transformation is one of the commonly used transformation technique used in these situations.\n",
    "\n",
    "![title](feature_img/fea1.png)\n",
    "\n",
    "Symmetric distribution is preferred over skewed distribution as it is easier to interpret and generate inferences. Some modeling techniques requires normal distribution of variables. So, whenever we have a skewed distribution, we can use transformations which reduce skewness. For right skewed distribution, we take square / cube root or logarithm of variable and for left skewed, we take square / cube or exponential of variables.\n",
    "\n",
    "\n",
    "![title](feature_img/fea2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the common methods of Variable Transformation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various methods used to transform variables. As discussed, some of them include square root, cube root, logarithmic, binning, reciprocal and many others. Let’s look at these methods in detail by highlighting the pros and cons of these transformation methods.\n",
    "\n",
    "•\tLogarithm: Log of a variable is a common transformation method used to change the shape of distribution of the variable on a distribution plot. It is generally used for reducing right skewness of variables. Though, It can’t be applied to zero or negative values as well.\n",
    "\n",
    "•\tSquare / Cube root: The square and cube root of a variable has a sound effect on variable distribution. However, it is not as significant as logarithmic transformation. Cube root has its own advantage. It can be applied to negative values including zero. Square root can be applied to positive values including zero.\n",
    "\n",
    "•\tBinning: It is used to categorize variables. It is performed on original values, percentile or frequency. Decision of categorization technique is based on business understanding. For example, we can categorize income in three categories, namely: High, Average and Low. We can also perform co-variate binning which depends on the value of more than one variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Feature / Variable Creation & its Benefits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature / Variable creation is a process to generate a new variables / features based on existing variable(s). For example, say, we have date(dd-mm-yy) as an input variable in a data set. We can generate new variables like day, month, year, week, weekday that may have better relationship with target variable. This step is used to highlight the hidden relationship in a variable:\n",
    "![title](feature_img/fea3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tCreating derived variables:\n",
    "\n",
    "This refers to creating new variables from existing variable(s) using set of functions or different methods. Let’s look at it through “Titanic – Kaggle competition”. In this data set, variable age has missing values. To predict missing values, we used the salutation (Master, Mr, Miss, Mrs) of name as a new variable. How do we decide which variable to create? Honestly, this depends on business understanding of the analyst, his curiosity and the set of hypothesis he might have about the problem. Methods such as taking log of variables, binning variables and other methods of variable transformation can also be used to create new variables.\n",
    "\n",
    "•\tCreating dummy variables:\n",
    "\n",
    "One of the most common application of dummy variable is to convert categorical variable into numerical variables. Dummy variables are also called Indicator Variables. It is useful to take categorical variable as a predictor in statistical models.  Categorical variable can take values 0 and 1. Let’s take a variable ‘gender’. We can produce two variables, namely, “Var_Male” with values 1 (Male) and 0 (No male) and “Var_Female” with values 1 (Female) and 0 (No Female). We can also create dummy variables for more than two classes of a categorical variables with n or n-1 dummy variables.\n",
    "\n",
    "![title](feature_img/fea4.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy Coding: Dummy coding is a commonly used method for converting a categorical input variable into continuous variable. 'Dummy', as the name suggests is a duplicatevariable which represents one level of a categorical variable. Presence of a level is represent by 1 and absence is represented by 0\n",
    "In a simple term,\n",
    "Let’s say, we have a data set with features X is [ID, Surname, Age, Country] as follows\n",
    " \n",
    "categorical column called “Country” and its values are - [India, Germany, France]\n",
    "In ML regression models, predictions will do the good job if categorical values are converted into numerical (binary vectors ) values. Encoding categorical data technique to apply for the above categorical set and the values a.k.a dummy variables will become\n",
    " \n",
    "This is called - One hot encoding technique.\n",
    "\n",
    "Dummy Variable Trap :\n",
    "With one hot encoding conversion, we have three columns in place. By including dummy variables in a regression model, we should consider to drop a column - “Dummy variable trap” N - 1 dummy variables (It is always a good practise to use minimal set of features to apply ML techniques and create regression model.) . In the above dummy variables table, we should consider to drop any of one columns. Let’s drop the column “Germany” and the final table looks like in below\n",
    " \n",
    "Thumb Rule :- Which dummy variable column do we need drop? The answer is - we can drop any of one dummy variables column. It can predict the dropped column’s value based on other two columns. Let’s take the record no 3 in the above table, both dummy variable values are ‘0’. So obviously another dummy variable column value is ‘1’ and categorical value is ‘Germany’.\n",
    "Technically, the dummy variable trap is a scenario in which the independent variables are multi-collinear - two or more variables are highly correlated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](feature_img/fea5.png)\n",
    "\n",
    "Binning can be applied on both categorical and numerical data:\n",
    "\n",
    "#Numerical Binning Example\n",
    "\n",
    "Value      Bin  \n",
    "\n",
    "0-30   ->  Low \n",
    "\n",
    "31-70  ->  Mid \n",
    "\n",
    "71-100 ->  High\n",
    "\n",
    "#Categorical Binning Example\n",
    "\n",
    "Value      Bin     \n",
    "\n",
    "Spain  ->  Europe \n",
    "\n",
    "Italy  ->  Europe \n",
    "\n",
    "Chile  ->  South America\n",
    "\n",
    "Brazil ->  South America\n",
    "\n",
    "The main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance. Every time you bin something, you sacrifice information and make your data more regularized. (Please see regularization in machine learning)\n",
    "\n",
    "The trade-off between performance and overfitting is the key point of the binning process. In my opinion, for numerical columns, except for some obvious overfitting cases, binning might be redundant for some kind of algorithms, due to its effect on model performance.\n",
    "\n",
    "However, for categorical columns, the labels with low frequencies probably affect the robustness of statistical models negatively. Thus, assigning a general category to these less frequent values helps to keep the robustness of the model. For example, if your data size is 100,000 rows, it might be a good option to unite the labels with a count less than 100 to a new category like “Other”.\n",
    "\n",
    "\n",
    "#Numerical Binning Example\n",
    "\n",
    "data['bin'] = pd.cut(data['value'], bins=[0,30,70,100], labels=[\"Low\", \"Mid\", \"High\"])\n",
    "\n",
    "   value   bin\n",
    "0      2   Low\n",
    "\n",
    "1     45   Mid\n",
    "\n",
    "2      7   Low\n",
    "\n",
    "3     85  High\n",
    "\n",
    "4     28   Low\n",
    "\n",
    "\n",
    "#Categorical Binning Example\n",
    "\n",
    "     Country\n",
    "     \n",
    "0      Spain\n",
    "\n",
    "1      Chile\n",
    "\n",
    "2  Australia\n",
    "\n",
    "3      Italy\n",
    "\n",
    "4     Brazil\n",
    "\n",
    "\n",
    "conditions = [\n",
    "    data['Country'].str.contains('Spain'),\n",
    "    data['Country'].str.contains('Italy'),\n",
    "    data['Country'].str.contains('Chile'),\n",
    "    data['Country'].str.contains('Brazil')]\n",
    "\n",
    "choices = ['Europe', 'Europe', 'South America', 'South America']\n",
    "\n",
    "data['Continent'] = np.select(conditions, choices, default='Other')\n",
    "     Country      Continent\n",
    "0      Spain         Europe\n",
    "1      Chile  South America\n",
    "2  Australia          Other\n",
    "3      Italy         Europe\n",
    "4     Brazil  South America\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is one of the most common encoding methods in machine learning. This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded column\n",
    "\n",
    "This method changes your categorical data, which is challenging to understand for algorithms, to a numerical format and enables you to group your categorical data without losing any information. (For details please see the last part of Categorical Column Grouping)\n",
    "\n",
    "![title](feature_img/fea6.png)\n",
    "\n",
    "Why One-Hot?: If you have N distinct values in the column, it is enough to map them to N-1 binary columns, because the missing value can be deducted from other columns. If all the columns in our hand are equal to 0, the missing value must be equal to 1. This is the reason why it is called as one-hot encoding. However, I will give an example using the get_dummies function of Pandas. This function maps all values in a column to multiple columns\n",
    "\n",
    "encoded_columns = pd.get_dummies(data['column'])\n",
    "\n",
    "data = data.join(encoded_columns).drop('column', axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the feature importance of each feature of your dataset by using the feature importance property of the model.\n",
    "\n",
    "Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.\n",
    "\n",
    "Feature importance is an inbuilt class that comes with Tree Based Classifiers, we will be using Extra Tree Classifier for extracting the top 10 features for the dataset.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"D://Blogs//train.csv\")\n",
    "\n",
    "X = data.iloc[:,0:20]  #independent columns\n",
    "\n",
    "y = data.iloc[:,-1]    #target column i.e price range\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "\n",
    "model.fit(X,y)\n",
    "\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "\n",
    "#plot graph of feature importances for better visualization\n",
    "\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "![title](feature_img/fea7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix with Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation states how the features are related to each other or the target variable.\n",
    "\n",
    "Correlation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)\n",
    "\n",
    "Heatmap makes it easy to identify which features are most related to the target variable, we will plot heatmap of correlated features using the seaborn library.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv(\"D://Blogs//train.csv\")\n",
    "\n",
    "X = data.iloc[:,0:20]  #independent columns\n",
    "\n",
    "y = data.iloc[:,-1]    #target column i.e price range\n",
    "\n",
    "#get correlations of each features in dataset\n",
    "\n",
    "corrmat = data.corr()\n",
    "\n",
    "top_corr_features = corrmat.index\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "#plot heat map\n",
    "\n",
    "g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
    "\n",
    "![title](feature_img/fea8.png)\n",
    "\n",
    "Have a look at the last row i.e price range, see how the price range is correlated with other features, ram is the highly correlated with price range followed by battery power, pixel height and width while m_dep, clock_speed and n_cores seems to be least correlated with price_rang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
