{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PCA.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB0ONr5TaMCv",
        "colab_type": "text"
      },
      "source": [
        "### Implementing PCA in Python with Scikit-Learn \n",
        "\n",
        "\n",
        "https://stackabuse.com/implementing-pca-in-python-with-scikit-learn/\n",
        "\n",
        "\n",
        "With the availability of high performance CPUs and GPUs, it is pretty much possible to solve every regression, classification, clustering and other related problems using machine learning and deep learning models. However, there are still various factors that cause performance bottlenecks while developing such models. Large number of features in the dataset is one of the factors that affect both the training time as well as accuracy of machine learning models. You have different options to deal with huge number of features in a dataset.\n",
        "\n",
        "1- Try to train the models on original number of features, which take days or weeks if the number of features is too high.\n",
        "\n",
        "2- Reduce the number of variables by merging correlated variables.\n",
        "\n",
        "3- Extract the most important features from the dataset that are responsible for maximum variance in the output. Different statistical techniques are used for this purpose e.g. linear discriminant analysis, factor analysis, and principal component analysis.\n",
        "\n",
        "In this article, we will see how principal component analysis can be implemented using Python's Scikit-Learn library.\n",
        "\n",
        "**Principal Component Analysis**\n",
        "Principal component analysis, or PCA, is a statistical technique to convert high dimensional data to low dimensional data by selecting the most important features that capture maximum information about the dataset. The features are selected on the basis of variance that they cause in the output. The feature that causes highest variance is the first principal component. The feature that is responsible for second highest variance is considered the second principal component, and so on. It is important to mention that principal components do not have any correlation with each other.\n",
        "\n",
        "**Advantages of PCA**\n",
        "There are two main advantages of dimensionality reduction with PCA.\n",
        "\n",
        "The training time of the algorithms reduces significantly with less number of features.\n",
        "It is not always possible to analyze data in high dimensions. For instance if there are 100 features in a dataset. Total number of scatter plots required to visualize the data would be 100(100-1)2 = 4950. Practically it is not possible to analyze data this way.\n",
        "\n",
        "**Normalization of Features**\n",
        "It is imperative to mention that a feature set must be normalized before applying PCA. For instance if a feature set has data expressed in units of Kilograms, Light years, or Millions, the variance scale is huge in the training set. If PCA is applied on such a feature set, the resultant loadings for features with high variance will also be large. Hence, principal components will be biased towards features with high variance, leading to false results.\n",
        "\n",
        "Finally, the last point to remember before we start coding is that PCA is a statistical technique and can only be applied to numeric data. Therefore, categorical features are required to be converted into numerical features before PCA can be applied.\n",
        "\n",
        "**Implementing PCA with Scikit-Learn**\n",
        "In this section we will implement PCA with the help of Python's Scikit-Learn library. We will follow the classic machine learning pipeline where we will first import libraries and dataset, perform exploratory data analysis and preprocessing, and finally train our models, make predictions and evaluate accuracies. The only additional step will be to perform PCA to find out optimal number of features before we train our models. These steps have been implemented as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIcBRqUhaYmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing Libraries\n",
        "import numpy as np  \n",
        "import pandas as pd  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtiqLns9bUMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Execute the following script to download the dataset using pandas:\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"  \n",
        "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']  \n",
        "dataset = pd.read_csv(url, names=names)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCJby50NbdQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8DQGMY8binF",
        "colab_type": "code",
        "outputId": "48812ed8-14d9-4aea-abea-1b5f7da88467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "dataset.head() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal-length</th>\n",
              "      <th>sepal-width</th>\n",
              "      <th>petal-length</th>\n",
              "      <th>petal-width</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal-length  sepal-width  petal-length  petal-width        Class\n",
              "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
              "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
              "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
              "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
              "4           5.0          3.6           1.4          0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89epf9SybqVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing\n",
        "# The first preprocessing step is to divide the dataset into a feature set and corresponding labels. The following script performs this task:\n",
        "# DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n",
        "\n",
        "X = dataset.drop('Class', 1) # axis : {0 or ‘index’, 1 or ‘columns’}, default 0 Whether to drop labels from the index (0 or ‘index’) or columns (1 or ‘columns’).\n",
        "y = dataset['Class'] # labels : single label or list-like Index or column labels to drop."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ugwhMvwdFq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check features\n",
        "\n",
        "X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFJ-ruT-cBwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check labels\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dkHzuU-c--f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The next preprocessing step is to divide data into training and test sets. Execute the following script to do so:\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8MHo06bfCmO",
        "colab_type": "text"
      },
      "source": [
        "#Feature Scaling \n",
        "\n",
        "Refer below link to get more info:\n",
        "\n",
        "https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e\n",
        "\n",
        "##Why Scaling:\n",
        "Most of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem.\n",
        "\n",
        "\n",
        "If left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.\n",
        "\n",
        "![alt text](![feature scaling](https://cdn-images-1.medium.com/max/1600/1*EyPd0sQxEXtTDSJgu72JNQ.jpeg))\n",
        "\n",
        "\n",
        "To supress this effect, we need to bring all features to the same level of magnitudes. This can be acheived by scaling.\n",
        "\n",
        "\n",
        "## How to Scale Features\n",
        "\n",
        "There are four common methods to perform Feature Scaling.\n",
        "\n",
        "**1- Standardisation**:\n",
        "\n",
        "**values will lie be between -1 and 1.**\n",
        "\n",
        "Standardization is one of the most popular methods for scaling features. It basically replaces the values with their Z scores.\n",
        "\n",
        "This method redistributes the features with their mean = 0 and standard deviation = 1.\n",
        "\n",
        "Scikit-Learn provides a preprocessing module that contains different preprocessing methods including standardization.\n",
        "\n",
        "Here is a simple code to demonstrate that.\n",
        "\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "Lets assume that we have a numpy array with some values\n",
        "And we want to scale the values of the array\n",
        "sc = scale(X)\n",
        "\n",
        "Standardisation replaces the values by their Z scores.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*LysCPCvg0AzQenGoarL_hQ.png)\n",
        "\n",
        "Standard Deviation:\n",
        "\n",
        "#### Please follow below link for more details\n",
        "https://www.mathsisfun.com/data/standard-deviation-formulas.html\n",
        "\n",
        "This is the formula for Standard Deviation:\n",
        "![alt text](https://www.mathsisfun.com/data/images/standard-deviation-formula.gif)\n",
        "\n",
        "StandardScaler makes the mean of the distribution 0. About 68% of the values will lie be between -1 and 1.\n",
        "\n",
        "\n",
        "**Scikit-learn: preprocessing.scale() vs preprocessing.StandardScalar()**\n",
        "\n",
        "Those are doing exactly the same, but:\n",
        "\n",
        "**preprocessing.scale(x) is just a function, which transforms some data**\n",
        "\n",
        "**preprocessing.StandardScaler() is a class supporting the Transformer API**\n",
        "\n",
        "I would always use the latter, even if i would not need inverse_transform and co. supported by StandardScaler().?\n",
        "\n",
        "\n",
        "Excerpt from the docs:\n",
        "\n",
        "**The function scale provides a quick and easy way to perform this operation on a single array-like dataset**\n",
        "\n",
        "**The preprocessing module further provides a utility class StandardScaler that implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. This class is hence suitable for use in the early steps of a sklearn.pipeline.Pipeline**\n",
        "\n",
        "\n",
        "**This redistributes the features with their mean μ = 0 and standard deviation σ =1 . sklearn.preprocessing.scale helps us implementing standardisation in python.**\n",
        "<hr>\n",
        "\n",
        "**2-  Mean Normalisation:**\n",
        "\n",
        "\n",
        "This distribution will have values between -1 and 1with μ=0.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/2400/1*fyK4gMQrfJKV5pmbXSrNbg.png)\n",
        "\n",
        "**Standardisation and Mean Normalization can be used for algorithms that assumes zero centric data like Principal Component Analysis(PCA).**\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "\n",
        "**3- Min-Max Scaling:**\n",
        "\n",
        "sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*19hq_t_NFQ6YVxMxsT0Cqg.png)\n",
        "\n",
        "This scaling brings the value between 0 and 1.\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "\n",
        "**4- Unit Vector:**\n",
        "\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*u2Up0eaer56dpmaElU3Zxw.png)\n",
        "\n",
        "Scaling is done considering the whole feature vecture to be of unit length.\n",
        "\n",
        "**Min-Max Scaling and Unit Vector techniques produces values of range [0,1]. When dealing with features with hard boundaries this is quite useful. For example, when dealing with image data, the colors can range from only 0 to 255**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##When to Scale\n",
        "\n",
        "Examples of Algorithms where Feature Scaling matters \n",
        "1. K-Means uses the Euclidean distance measure here feature scaling matters.\n",
        "2. K-Nearest-Neighbours also require feature scaling.\n",
        "3. Principal Component Analysis (PCA): Tries to get the feature with maximum variance, here too feature scaling is required.\n",
        "4. Gradient Descent: Calculation speed increase as Theta calculation becomes faster after feature scaling.\n",
        "\n",
        "**Note: Naive Bayes, Linear Discriminant Analysis, and Tree-Based models are not affected by feature scaling.**\n",
        "**In Short, any Algorithm which is Not Distance based is Not affected by Feature Scaling.**\n",
        "\n",
        "\n",
        "Rule of thumb I follow here is any algorithm that computes distance or assumes normality, scale your features!!!\n",
        "\n",
        "Some examples of algorithms where feature scaling matters are:\n",
        "\n",
        "k-nearest neighbors with an Euclidean distance measure is sensitive to magnitudes and hence should be scaled for all features to weigh in equally.\n",
        "\n",
        "Scaling is critical, while performing Principal Component Analysis(PCA). PCA tries to get the features with maximum variance and the variance is high for high magnitude features. This skews the PCA towards high magnitude features.\n",
        "\n",
        "We can speed up gradient descent by scaling. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
        "\n",
        "Tree based models are not distance based models and can handle varying ranges of features. Hence, Scaling is not required while modelling trees\n",
        ".\n",
        "Algorithms like Linear Discriminant Analysis(LDA), Naive Bayes are by design equipped to handle this and gives weights to the features accordingly. Performing a features scaling in these algorithms may not have much effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGig-52tpe5z",
        "colab_type": "text"
      },
      "source": [
        "# Distance can be calculated between centroid and data point using these methods-\n",
        "\n",
        "1 -Euclidean Distance : It is the square-root of the sum of squares of differences between the coordinates (feature values – Age, Salary, BHK Apartment) of data point and centroid of each class. This formula is given by Pythagorean theorem.\n",
        "\n",
        "![alt text](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/mink-dis.jpg)\n",
        "\n",
        "where x is Data Point value, y is Centroid value and k is no. of feature values, Example: given data set has k = 3\n",
        "\n",
        "\n",
        "2- Manhattan Distance : It is calculated as the sum of absolute differences between the coordinates (feature values) of data point and centroid of each class.\n",
        "\n",
        "![alt text](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/manh-dis.jpg)\n",
        "\n",
        "3- Minkowski Distance : It is a generalization of above two methods. As shown in the figure, different values can be used for finding r.\n",
        "\n",
        "![alt text](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/mink-dis.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCZ8olkcqBKD",
        "colab_type": "text"
      },
      "source": [
        "# Handling Missing Data\n",
        "\n",
        "Handling Missing Data is very common in Machine Learning. It means your dataset does not contain any information for a certain feature in a specific row. Almost all datasets come with some missing values.\n",
        "We know that Machine Learning algorithms are just some math equations. That means we cannot throw these empty (missing) values into those algorithms.\n",
        "\n",
        "**There are two very commonly used methods for Handling Missing Data.**\n",
        "1. Removing Data\n",
        "2. Imputation\n",
        "\n",
        "\n",
        "**1 - Removing Data**\n",
        "In many cases, the solution is just removing the specific row. If we use pandas, then it is very simple. We just need to use one pandas methods called dropna().\n",
        "Let’s say we have a pandas DataFrame df. This DataFrame contains some missing values, then we can write a simple code to delete those specific rows.\n",
        "\n",
        "new_df = df.dropna(axis=1)\n",
        "\n",
        "But this approach is not the best solution. A better solution to this problem is Imputation.\n",
        "\n",
        "**2- Imputation**\n",
        "Imputation is another very popular methods for handling missing values. In Imputation, instead of deleting the rows, we fill the rows with some other values. The imputed value is not the right number but it is very accurate to the right value.\n",
        "\n",
        "Scikit-Learn provides for methods for Imputation. We can use the SimpleImputer method for Imputation. Here is code for that\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy=’mean’)\n",
        "new_data = imp.fit_transform(data)\n",
        "\n",
        "\n",
        "SimpleImputer supports different strategies for imputation. In the above example, we have used mean strategy. That means this method replaces the missing values with the mean value for that column. We can also use a constant value for as a strategy so that it’ll replace all the missing values with a constant value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjI0LYebrTvO",
        "colab_type": "text"
      },
      "source": [
        "# Categorical Data\n",
        "\n",
        "**Data Encoding**\n",
        "We know that Machine Learning algorithms require data in numerical form. But many times, datasets contains some features in some other form. So we need to convert these value into numerical form.\n",
        "Scikit-Learn provides different encoding methods for Data Encoding.\n",
        "\n",
        "\n",
        "**Label Encoding**\n",
        "\n",
        "To understand Label Encoding, first, let’s assume a dataset contains three columns age, salary, and gender. Now in this dataset, the gender column is not in numerical form. That means we need to convert it into some type of numerical form.\n",
        "\n",
        "\n",
        "To achieve that, we can use Label Encoding. We know that the gender columns contains two unique values, Male and Female. If we apply Label Encoding algorithm on this column, then it replaces the values by 0 and 1 (Male = 0 and Female = 1).\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "x = labelencoder.fit_transform(x).toarray()\n",
        "\n",
        "**One Hot Encoding**\n",
        "\n",
        "Label Encoding sometimes causes different problems. Sometimes by using Label Encoding, the Machine Learning algorithm may confuse and assume that the data have some type of hierarchical order. To avoid this, we can use One Hot Encoding.\n",
        "What one hot encoding does is, it takes a column which has categorical data, which has been label encoded and then splits the column into multiple columns. The numbers are replaced by 1s and 0s, depending on which column has what value.\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
        "x = onehotencoder.fit_transform(x).toarray()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjX2RHlVdmtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As mentioned earlier, PCA performs best with a normalized feature set. We will perform standard scalar normalization to normalize our feature set. To do this, execute the following code:\n",
        "\n",
        "# standardization technique\n",
        "# https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()  \n",
        "X_train = sc.fit_transform(X_train)  \n",
        "X_test = sc.transform(X_test) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rIilu-fRcQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNWAL3KUtDsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Applying PCA\n",
        "#It is only a matter of three lines of code to perform PCA using Python's Scikit-Learn library. The PCA class is used for this purpose. PCA depends only upon the feature set and not the label data. Therefore, PCA can be considered as an unsupervised machine learning technique.\n",
        "#Performing PCA using Scikit-Learn is a two-step process:\n",
        "#Initialize the PCA class by passing the number of components to the constructor.\n",
        "#Call the fit and then transform methods by passing the feature set to these methods. The transform method returns the specified number of principal components.\n",
        "#Take a look at the following code:\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()  \n",
        "X_train = pca.fit_transform(X_train)  \n",
        "X_test = pca.transform(X_test)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlT7IAmWRrD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISktilZ-teJO",
        "colab_type": "code",
        "outputId": "e0d0beba-c3c0-4bef-fbac-aa8d6306f0c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# In the code above, we create a PCA object named pca. We did not specify the number of components in the constructor. Hence, all four of the features in the feature set will be returned for both the training and test sets.\n",
        "# The PCA class contains explained_variance_ratio_ which returns the variance caused by each of the principal components. Execute the following line of code to find the \"explained variance ratio\".\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "explained_variance"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.72226528, 0.23974795, 0.03338117, 0.0046056 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhkc3Yhtto52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's first try to use 1 principal component to train our algorithm. To do so, execute the following code:\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=1)  \n",
        "X_train = pca.fit_transform(X_train)  \n",
        "X_test = pca.transform(X_test)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhm0Kk3_tt7W",
        "colab_type": "code",
        "outputId": "3b3335f0-faa1-4e1d-f270-d81e0dccdad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Training and Making Predictions\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "classifier = RandomForestClassifier(max_depth=2, random_state=0)  \n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd2Fx4fetyxX",
        "colab_type": "code",
        "outputId": "49d65998-2024-43c5-f101-4c80f12ad84b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Performance Evaluation\n",
        "from sklearn.metrics import confusion_matrix  \n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)  \n",
        "print(cm)  \n",
        "print('Accuracy' + str(accuracy_score(y_test, y_pred)))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[11  0  0]\n",
            " [ 0 12  1]\n",
            " [ 0  1  5]]\n",
            "Accuracy0.9333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s9rpGLet5zH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Results with 2 and 3 Principal Components\n",
        "# Now let's try to evaluate classification performance of the random forest algorithm with 2 principal components. Update this piece of code:\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)  \n",
        "X_train_1 = pca.fit_transform(X_train)  \n",
        "X_test_1 = pca.transform(X_test)  \n",
        "\n",
        "          \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}