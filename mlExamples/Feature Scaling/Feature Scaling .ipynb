{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition\n",
    "\n",
    "\"It is a step of Data Pre Processing which is applied to independent variables or features of data. It basically helps to normalise the data within a particular range. Sometimes, it also helps in speeding up the calculations in an algorithm.\"\n",
    "\n",
    "![title](feature_scale_img/fs1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem.\n",
    "\n",
    "\n",
    "If left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Scale Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four common methods to perform Feature Scaling.\n",
    "\n",
    "1.\tStandardisation:\n",
    "\n",
    "![title](feature_scale_img/fs2.png)\n",
    "\n",
    "Standardisation replaces the values by their Z scores.\n",
    "\n",
    "This redistributes the features with their mean μ = 0 and standard deviation σ =1 . sklearn.preprocessing.scale helps us implementing standardisation in python\n",
    "\n",
    "2. Mean Normalisation:\n",
    "![title](feature_scale_img/fs3.png)\n",
    "This distribution will have values between -1 and 1with μ=0.\n",
    "\n",
    "Standardisation and Mean Normalization can be used for algorithms that assumes zero centric data like Principal Component Analysis(PCA).\n",
    "\n",
    "3. Min-Max Scaling:\n",
    "![title](feature_scale_img/fs4.png)\n",
    "This scaling brings the value between 0 and 1.\n",
    "\n",
    "4. Unit Vector:\n",
    "\n",
    "![title](feature_scale_img/fs5.png)\n",
    "Scaling is done considering the whole feature vecture to be of unit length\n",
    "\n",
    "\n",
    "Min-Max Scaling and Unit Vector techniques produces values of range [0,1]. When dealing with features with hard boundaries this is quite useful. For example, when dealing with image data, the colors can range from only 0 to 255.\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule of thumb I follow here is any algorithm that computes distance or assumes normality, scale your features!!!\n",
    "\n",
    "Some examples of algorithms where feature scaling matters are:\n",
    "\n",
    "•\tk-nearest neighbors with an Euclidean distance measure is sensitive to magnitudes and hence should be scaled for all features to weigh in equally.\n",
    "\n",
    "•\tScaling is critical, while performing Principal Component Analysis(PCA). PCA tries to get the features with maximum variance and the variance is high for high magnitude features. This skews the PCA towards high magnitude features.\n",
    "\n",
    "\n",
    "•\tWe can speed up gradient descent by scaling. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "\n",
    "\n",
    "•\tTree based models are not distance based models and can handle varying ranges of features. Hence, Scaling is not required while modelling trees.\n",
    "\n",
    "\n",
    "•\tAlgorithms like Linear Discriminant Analysis(LDA), Naive Bayes are by design equipped to handle this and gives weights to the features accordingly. Performing a features scaling in these algorithms may not have much effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler\n",
    "\n",
    "importpandas as pd \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "  \n",
    "#################Read Data from CSV \n",
    "\n",
    "data = read_csv('standardscaler_file.csv') \n",
    "data.head() \n",
    "  \n",
    "#################Initialise the Scaler\n",
    "\n",
    "scaler = StandardScaler() \n",
    "  \n",
    "#################To scale data \n",
    "\n",
    "scaler.fit(data) \n",
    "\n",
    "##########While many algorithms (such as SVM, K-nearest neighbors, and logistic regression) require features to be normalized\n",
    "\n",
    "Below is dataset where we can apply StandardScaler method:\n",
    "\n",
    "![title](feature_scale_img/fs6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Max Scaler\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range =(0, 1)) \n",
    "\n",
    "x_after_min_max_scaler = min_max_scaler.fit_transform(x) \n",
    "\n",
    "print (\"\\nAfter min max Scaling : \\n\", x_after_min_max_scaler) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
