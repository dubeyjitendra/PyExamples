{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in.\n",
    "\n",
    "Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more details please check below link:\n",
    "\n",
    "https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e    \n",
    "https://towardsdatascience.com/why-how-and-when-to-apply-feature-selection-e9c69adfabf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will focus on one of the 2 critical parts of getting your models right – feature selection. I will discuss in detail why feature selection plays such a vital role in creating an effective predictive model.\n",
    "\n",
    "Point which we will cover in Feature Selection:\n",
    "\n",
    "1.\tImportance of Feature Selection in Machine Learning\n",
    "2.\tFilter Methods\n",
    "3.\tWrapper Methods\n",
    "4.\tEmbedded Methods\n",
    "5.\tDifference between Filter and Wrapper methods\n",
    "6.\tWalkthrough example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of Feature Selection in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning works on a simple rule – if you put garbage in, you will only get garbage to come out. By garbage here, I mean noise in data.\n",
    "\n",
    "This becomes even more important when the number of features are very large. You need not use every feature at your disposal for creating an algorithm. You can assist your algorithm by feeding in only those features that are really important. I have myself witnessed feature subsets giving better results than complete set of feature for the same algorithm. Or as Rohan Rao puts it – “Sometimes, less is better!”\n",
    "Not only in the competitions but this can be very useful in industrial applications as well. You not only reduce the training time and the evaluation time, you also have less things to worry about\n",
    "\n",
    "Top reasons to use feature selection are:\n",
    "\n",
    "•\tIt enables the machine learning algorithm to train faster.\n",
    "\n",
    "•\tIt reduces the complexity of a model and makes it easier to interpret.\n",
    "\n",
    "•\tIt improves the accuracy of a model if the right subset is chosen.\n",
    "\n",
    "•\tIt reduces overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "# Scikit learn provides the Selecting K best features using F-Test\n",
    "sklearn.feature_selection.f_regression\n",
    "\n",
    "#For Classification tasks\n",
    "sklearn.feature_selection.f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods\n",
    "\n",
    "Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The correlation is a subjective term here. For basic guidance, you can refer to the following table for defining correlation co-efficients.\n",
    "\n",
    "![title](feature-selection/feat1.png)\n",
    "![title](feature-selection/feat2.png)\n",
    "![title](feature-selection/feat3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](feature-selection/feat4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "\n",
    "\n",
    " Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc.\n",
    " \n",
    "•\tForward Selection: Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "\n",
    "•\tBackward Elimination: In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n",
    "\n",
    "•\tRecursive Feature elimination: It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.\n",
    "\n",
    "\n",
    "One of the best ways for implementing feature selection with wrapper methods is to use Boruta package that finds the importance of a feature by creating shadow features.\n",
    "It works in the following steps:\n",
    "\n",
    "1.\tFirstly, it adds randomness to the given data set by creating shuffled copies of all features (which are called shadow features).\n",
    "\n",
    "2.\tThen, it trains a random forest classifier on the extended data set and applies a feature importance measure (the default is Mean Decrease Accuracy) to evaluate the importance of each feature where higher means more important.\n",
    "\n",
    "3.\tAt every iteration, it checks whether a real feature has a higher importance than the best of its shadow features (i.e. whether the feature has a higher Z-score than the maximum Z-score of its shadow features) and constantly removes features which are deemed highly unimportant.\n",
    "\n",
    "\n",
    "4.\tFinally, the algorithm stops either when all features get confirmed or rejected or it reaches a specified limit of random forest runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded Methods\n",
    "\n",
    "![title](feature-selection/feat5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded methods combine the qualities’ of filter and wrapper methods. It’s implemented by algorithms that have their own built-in feature selection methods.\n",
    "\n",
    "Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt penalization functions to reduce overfitting.\n",
    "\n",
    "•\tLasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients.\n",
    "\n",
    "•\tRidge regression performs L2 regularization which adds penalty equivalent to square of the magnitude of coefficients.\n",
    "For more details and implementation of LASSO and RIDGE regression, you can refer to this article.\n",
    "\n",
    "Other examples of embedded methods are Regularized trees, Memetic algorithm, Random multinomial logit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to select features and what are Benefits of performing feature selection before modeling your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n",
    "    \n",
    "• Improves Accuracy: Less misleading data means modeling accuracy improves.\n",
    "    \n",
    "• Reduces Training Time: fewer data points reduce algorithm complexity and algorithms train faster.\n",
    "    \n",
    "I want to share my personal experience with this.\n",
    "\n",
    "I prepared a model by selecting all the features and I got an accuracy of around 65% which is not pretty good for a predictive model and after doing some feature selection and feature engineering without doing any logical changes in my model code my accuracy jumped to 81% which is quite impressive\n",
    "Now you know why I say feature selection should be the first and most important step of your model design.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
