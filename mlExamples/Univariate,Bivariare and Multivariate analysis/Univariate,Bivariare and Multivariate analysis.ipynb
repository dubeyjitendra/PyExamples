{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Identification\n",
    "\n",
    "    First of all, identify feature (such as Input) and label (such as output) variables. Next, \n",
    "       identify the \"data type\" and \"category\" of the variables.\n",
    "       \n",
    "     Example:- In below image, we need to predict the students will play cricket or not \n",
    "     and also identfy entire view of dataset\n",
    "     \n",
    "![title](img/image-value.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis \n",
    "\n",
    "This analysis is performing only for one variable. \n",
    "\n",
    "For Numerical values :\n",
    "\n",
    "    1-Mean\n",
    "    2-median\n",
    "    3-mode\n",
    "    4-range\n",
    "    5-percentiles\n",
    "    6-Scatterplot\n",
    "    7-Frequencies\n",
    "    8-Histogram\n",
    "    9-Skewness\n",
    "    10- kurtosis\n",
    "    11-min\n",
    "    12-max\n",
    "    13-variance\n",
    "    14-Standard Deviation\n",
    "    \n",
    "For Categorical values:\n",
    "\n",
    "    1- Count\n",
    "    2- Count%\n",
    "    3- Bar Chart\n",
    "    \n",
    "    \n",
    " Please check my other notebook to get more details:\n",
    " https://github.com/dubeyjitendra/Machine-Learning/blob/master/Statistical/Statistical-part-1.ipynb\n",
    " https://github.com/dubeyjitendra/Machine-Learning/blob/master/Machine-Learning-Preprocessing-Part-1/Skewness.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Analysis \n",
    "\n",
    "Bi-variate Analysis finds out the relationship between two variables\n",
    "we look for association and disassociation between variables at a pre-defined significance level\n",
    "We can perform bi-variate analysis for any combination of categorical and continuous variables\n",
    "combination are:\n",
    "    1-Categorical & Categorical \n",
    "    2-Categorical & Continuous\n",
    "    3-Continuous & Continuous\n",
    "    \n",
    "\n",
    " 1-Continuous & Continuous: \n",
    "     While doing bi-variate analysis between two continuous variables, we should look at scatter plot. It is a nifty way to find out the relationship between two variables. The pattern of scatter plot indicates the relationship between variables. The relationship can be linear or non-linear.\n",
    "\n",
    "    a)- Scatter plot shows the relationship between two variable but does not indicates the strength of relationship amongst them\n",
    "    b)- we use Correlation. Correlation varies between -1 and +1\n",
    "\n",
    "        1)     -1: perfect negative linear correlation\n",
    "        2)     +1:perfect positive linear correlation \n",
    "        3)      0: No correlation\n",
    "        \n",
    "##### Correlation can be derived using following formula:\n",
    "Correlation = Covariance(X,Y) / SQRT( Var(X)* Var(Y))\n",
    "![title](img/continuos.png)\n",
    "![title](img/continuous-1.png)\n",
    "\n",
    "we have good positive relationship(0.65) between two variables X and Y.\n",
    "\n",
    "2- Categorical & Categorical\n",
    "\n",
    "To find the relationship between two categorical variables, we can use following methods:\n",
    "    a)- Two-way table : \n",
    "        1- We can start analyzing the relationship by creating a two-way table of count and count%. The rows represents the category of one variable and the columns represent the categories of the other variable\n",
    "        2- We show count or count% of observations available in each combination of row and column categories.\n",
    "    \n",
    "    b)- Stacked Column Chart: ![title](img/cat-2.png)\n",
    "\n",
    "    c)- Chi-Square Test: This test is used to derive the statistical significance of relationship between the variables. Also, it tests whether the evidence in the sample is strong enough to generalize that the relationship for a larger population as well. Chi-square is based on the difference between the expected and observed frequencies in one or more categories in the two-way table. It returns probability for the computed chi-square distribution with the degree of freedom.\n",
    "chi square \n",
    "https://stattrek.com/online-calculator/chi-square.aspx\n",
    "\n",
    "    1- Probability of 0: It indicates that both categorical variable are dependent\n",
    "    2- Probability of 1: It shows that both variables are independent\n",
    "    3- Probability less than 0.05\n",
    "    \n",
    "    \n",
    "    It indicates that the relationship between the variables is significant at 95% confidence. The chi-square test statistic for a test of independence of two categorical variables is found by\n",
    "    \n",
    "   where O represents the observed frequency. E is the expected frequency under the null hypothesis and computed by\n",
    "    \n",
    "   ![title](img/cat-3.png)\n",
    "   \n",
    "   \n",
    "3-Categorical & Continuous:\n",
    "   \n",
    "        1-While exploring relation between categorical and continuous variables, we can draw box plots for each level of categorical variables.\n",
    "        2-If levels are small in number, it will not show the statistical significance. \n",
    "        3- To look at the statistical significance we can perform Z-test, T-test or ANOVA\n",
    "        \n",
    "    a)- Z-Test/ T-Test:\n",
    "   \n",
    "    Either test assess whether mean of two groups are statistically different from each other or not\n",
    "    If the probability of Z is small then the difference of two averages is more significant. The T-test is very similar to   Z-test but it is used when number of observation for both categories is less than 30.\n",
    "  \n",
    "  For Z-testin more depth: https://newonlinecourses.science.psu.edu/stat414/node/269/\n",
    "                            https://study.com/academy/lesson/z-test-formula-example.html\n",
    "                            \n",
    "  For -test https://newonlinecourses.science.psu.edu/stat414/node/270/\n",
    "   \n",
    "![title](img/z-test.png)\n",
    "         b)- ANNOVA:It assesses whether the average of more than two groups is statistically different\n",
    "\n",
    "                Example: Suppose, we want to test the effect of five different exercises. For this, we recruit 20 men and assign one type of exercise to 4 men (5 groups). Their weights are recorded after a few weeks. We need to find out whether the effect of these exercises on them is significantly different or not. This can be done by comparing the weights of the 5 groups of 4 men each\n",
    "\n",
    "    links: https://newonlinecourses.science.psu.edu/stat502/node/137/\n",
    "           https://explorable.com/anova\n",
    "\n",
    "\n",
    "    1-Crosstab : https://humansofdata.atlan.com/2016/01/cross-tabulation-how-why/\n",
    "            \n",
    "            A crosstab is a table showing the relationship between two or more variables. \n",
    "        \n",
    "    Cross tabulation is usually performed on categorical data — data that can be divided into mutually exclusive groups.\n",
    "    One simple way to do cross tabulations is Microsoft Excel’s pivot table feature. Pivot tables are a great way to search for patterns as they help in easily grouping raw data.\n",
    "    \n",
    "    \n",
    "    2-Chi-square\n",
    "    3- anovas\n",
    "    4- T-tests\n",
    "    5- z-test\n",
    "    6- Correlation\n",
    "    7- simple regression\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi variate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate analysis is a set of techniques used for analysis of data sets that contain more than two variable or atleast 3 variables, and the techniques are especially valuable when working with correlated variables.\n",
    "\n",
    "multivariate analysis is a tool to find patterns and relationships between several variables simultaneously. It lets us predict the effect a change in one variable will have on other variables.\n",
    "\n",
    "techniques are especially valuable when working with correlated variables.\n",
    "\n",
    "\n",
    "Some technique to use multi variate analysis:\n",
    "    1- Multiple Regression Analysis:\n",
    "    \n",
    "    Multiple regression is the most commonly utilized multivariate technique. It examines the relationship between a single metric dependent variable and two or more metric independent variables. The technique relies upon determining the linear relationship with the lowest sum of squared variances; therefore, assumptions of normality, linearity, and equal variance are carefully observed. The beta coefficients (weights) are the marginal impacts of each variable, and the size of the weight can be interpreted directly. Multiple regression is often used as a forecasting tool.\n",
    "    \n",
    "    2- Logistic Regression Analysis\n",
    "    \n",
    "    Sometimes referred to as “choice models,” this technique is a variation of multiple regression that allows for the prediction of an event. It is allowable to utilize nonmetric (typically binary) dependent variables, as the objective is to arrive at a probabilistic assessment of a binary choice. The independent variables can be either discrete or continuous. A contingency table is produced, which shows the classification of observations as to whether the observed and predicted events match. The sum of events that were predicted to occur which actually did occur and the events that were predicted not to occur which actually did not occur, divided by the total number of events, is a measure of the effectiveness of the model. This tool helps predict the choices consumers might make when presented with alternatives.\n",
    "    \n",
    "    3- Discriminant Analysis\n",
    "    \n",
    "    The purpose of discriminant analysis is to correctly classify observations or people into homogeneous groups. The independent variables must be metric and must have a high degree of normality. Discriminant analysis builds a linear discriminant function, which can then be used to classify the observations. The overall fit is assessed by looking at the degree to which the group means differ (Wilkes Lambda or D2) and how well the model classifies. To determine which variables have the most impact on the discriminant function, it is possible to look at partial F values. The higher the partial F, the more impact that variable has on the discriminant function. This tool helps categorize people, like buyers and nonbuyers.\n",
    "    \n",
    "    4- Multivariate Analysis of Variance (MANOVA)\n",
    "    \n",
    "    This technique examines the relationship between several categorical independent variables and two or more metric dependent variables. Whereas analysis of variance (ANOVA) assesses the differences between groups (by using T tests for two means and F tests between three or more means), MANOVA examines the dependence relationship between a set of dependent measures across a set of groups. Typically this analysis is used in experimental design, and usually a hypothesized relationship between dependent measures is used. This technique is slightly different in that the independent variables are categorical and the dependent variable is metric. Sample size is an issue, with 15-20 observations needed per cell. However, too many observations per cell (over 30) and the technique loses its practical significance. Cell sizes should be roughly equal, with the largest cell having less than 1.5 times the observations of the smallest cell. That is because, in this technique, normality of the dependent variables is important. The model fit is determined by examining mean vector equivalents across groups. If there is a significant difference in the means, the null hypothesis can be rejected and treatment differences can be determined.\n",
    "    \n",
    "    5- Factor Analysis:\n",
    "    \n",
    "    When there are many variables in a research design, it is often helpful to reduce the variables to a smaller set of factors. This is an independence technique, in which there is no dependent variable. Rather, the researcher is looking for the underlying structure of the data matrix. Ideally, the independent variables are normal and continuous, with at least three to five variables loading onto a factor. The sample size should be over 50 observations, with over five observations per variable. Multicollinearity is generally preferred between the variables, as the correlations are key to data reduction. Kaiser’s Measure of Statistical Adequacy (MSA) is a measure of the degree to which every variable can be predicted by all other variables. An overall MSA of .80 or higher is very good, with a measure of under .50 deemed poor. \n",
    "\n",
    "There are two main factor analysis methods: common factor analysis, which extracts factors based on the variance shared by the factors, and principal component analysis, which extracts factors based on the total variance of the factors. Common factor analysis is used to look for the latent (underlying) factors, whereas principal component analysis is used to find the fewest number of variables that explain the most variance. The first factor extracted explains the most variance. Typically, factors are extracted as long as the eigenvalues are greater than 1.0 or the Scree test visually indicates how many factors to extract. The factor loadings are the correlations between the factor and the variables. Typically a factor loading of .4 or higher is required to attribute a specific variable to a factor. An orthogonal rotation assumes no correlation between the factors, whereas an oblique rotation is used when some relationship is believed to exist.\n",
    "\n",
    "    6- Cluster Analysis\n",
    "    The purpose of cluster analysis is to reduce a large data set to meaningful subgroups of individuals or objects. The division is accomplished on the basis of similarity of the objects across a set of specified characteristics. Outliers are a problem with this technique, often caused by too many irrelevant variables. The sample should be representative of the population, and it is desirable to have uncorrelated factors. There are three main clustering methods: hierarchical, which is a treelike process appropriate for smaller data sets; nonhierarchical, which requires specification of the number of clusters a priori; and a combination of both. There are four main rules for developing clusters: the clusters should be different, they should be reachable, they should be measurable, and the clusters should be profitable (big enough to matter). This is a great tool for market segmentation.\n",
    "    \n",
    "    \n",
    "    7- Multidimensional Scaling (MDS)\n",
    "    \n",
    "    The purpose of MDS is to transform consumer judgments of similarity into distances represented in multidimensional space. This is a decompositional approach that uses perceptual mapping to present the dimensions. As an exploratory technique, it is useful in examining unrecognized dimensions about products and in uncovering comparative evaluations of products when the basis for comparison is unknown. Typically there must be at least four times as many objects being evaluated as dimensions. It is possible to evaluate the objects with nonmetric preference rankings or metric similarities (paired comparison) ratings. Kruskal’s Stress measure is a “badness of fit” measure; a stress percentage of 0 indicates a perfect fit, and over 20% is a poor fit. The dimensions can be interpreted either subjectively by letting the respondents identify the dimensions or objectively by the researcher.\n",
    "    \n",
    "    8- Correspondence Analysis\n",
    "    This technique provides for dimensional reduction of object ratings on a set of attributes, resulting in a perceptual map of the ratings. However, unlike MDS, both independent variables and dependent variables are examined at the same time. This technique is more similar in nature to factor analysis. It is a compositional technique, and is useful when there are many attributes and many companies. It is most often used in assessing the effectiveness of advertising campaigns. It is also used when the attributes are too similar for factor analysis to be meaningful. The main structural approach is the development of a contingency (crosstab) table. This means that the form of the variables should be nonmetric. The model can be assessed by examining the Chi-square value for the model. Correspondence analysis is difficult to interpret, as the dimensions are a combination of independent and dependent variables.\n",
    "    \n",
    "    9- Conjoint Analysis\n",
    "    \n",
    "    Conjoint analysis is often referred to as “trade-off analysis,” since it allows for the evaluation of objects and the various levels of the attributes to be examined. It is both a compositional technique and a dependence technique, in that a level of preference for a combination of attributes and levels is developed. A part-worth, or utility, is calculated for each level of each attribute, and combinations of attributes at specific levels are summed to develop the overall preference for the attribute at each level. Models can be built that identify the ideal levels and combinations of attributes for products and services.\n",
    "    \n",
    "    10 -Canonical Correlation\n",
    "    \n",
    "    The most flexible of the multivariate techniques, canonical correlation simultaneously correlates several independent variables and several dependent variables. This powerful technique utilizes metric independent variables, unlike MANOVA, such as sales, satisfaction levels, and usage levels. It can also utilize nonmetric categorical variables. This technique has the fewest restrictions of any of the multivariate techniques, so the results should be interpreted with caution due to the relaxed assumptions. Often, the dependent variables are related, and the independent variables are related, so finding a relationship is difficult without a technique like canonical correlation.\n",
    "    \n",
    "    11- Structural Equation Modeling\n",
    "    \n",
    "    Unlike the other multivariate techniques discussed, structural equation modeling (SEM) examines multiple relationships between sets of variables simultaneously. This represents a family of techniques, including LISREL, latent variable analysis, and confirmatory factor analysis. SEM can incorporate latent variables, which either are not or cannot be measured directly into the analysis. For example, intelligence levels can only be inferred, with direct measurement of variables like test scores, level of education, grade point average, and other related measures. These tools are often used to evaluate many scaled attributes or to build summated scales.\n",
    "\n",
    "\n",
    "To get More details, please refer below link \n",
    "https://www.decisionanalyst.com/whitepapers/multivariate/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
