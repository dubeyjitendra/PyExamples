{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Word Embedding is one such technique where we can represent the text using vectors.\n",
    "\n",
    "The more popular forms of word embeddings are:\n",
    "\n",
    "1- BoW, which stands for Bag of Words\n",
    "2- TF-IDF, which stands for Term Frequency-Inverse Document Frequency\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Basic feature extraction using text data\n",
    "Number of words\n",
    "\n",
    "Number of characters\n",
    "\n",
    "Average word length\n",
    "\n",
    "Number of stopwords\n",
    "\n",
    "Number of special characters\n",
    "\n",
    "Number of numerics\n",
    "\n",
    "Number of uppercase words\n",
    "\n",
    "#### Basic Text Pre-processing of text data\n",
    "Lower casing\n",
    "\n",
    "Punctuation removal\n",
    "\n",
    "Stopwords removal\n",
    "\n",
    "Frequent words removal\n",
    "\n",
    "Rare words removal\n",
    "\n",
    "Spelling correction\n",
    "\n",
    "Tokenization\n",
    "\n",
    "Stemming\n",
    "\n",
    "Lemmatization\n",
    "\n",
    "#### Advance Text Processing\n",
    "\n",
    "N-grams\n",
    "\n",
    "Term Frequency\n",
    "\n",
    "Inverse Document Frequency\n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "Bag of Words\n",
    "\n",
    "Sentiment Analysis\n",
    "\n",
    "Word Embedding\n",
    "\n",
    "refer this link:\n",
    "https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "It follows below steps :\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*ZJykVXi_OQRULpK6PlGNPQ.png\" width=\"550px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bag of Words (BoW) model is the simplest form of text representation in numbers.\n",
    "\n",
    "we can represent a sentence as a bag of words vector (a string of numbers).\n",
    "\n",
    "##### Eg\n",
    "\n",
    "Review 1: This movie is very scary and long\n",
    "Review 2: This movie is not scary and is slow\n",
    "Review 3: This movie is spooky and good\n",
    "\n",
    "\n",
    "Three types of movie reviews below:\n",
    "\n",
    "###### Review 1: This movie is very scary and long\n",
    "\n",
    "###### Review 2: This movie is not scary and is slow\n",
    "\n",
    "###### Review 3: This movie is spooky and good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====================================\n",
    "###### We will first build a vocabulary from all the unique words in the above three reviews. \n",
    "###### The vocabulary consists of these 11 words: ‘This’, ‘movie’, ‘is’, ‘very’, ‘scary’, ‘and’, ‘long’, ‘not’,  ‘slow’, ‘spooky’,  ‘good’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "##### We have 3 sentence above and we have created unique words of thats three sentences, after that we will create table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/BoWBag-of-Words-model-2.png\" width=\"550px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================================================\n",
    "\n",
    "Vector of Review 1: [1 1 1 1 1 1 1 0 0 0 0]\n",
    "\n",
    "Vector of Review 2: [1 1 2 0 0 1 1 0 1 0 0]\n",
    "\n",
    "Vector of Review 3: [1 1 1 0 0 0 1 0 0 1 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks of using a Bag-of-Words (BoW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-  it assumes all words are independent of each other.\n",
    "\n",
    "2-  It leads to a highly sparse vectors\n",
    "\n",
    "3-  Bag of words leads to a high dimensional feature vector due to large size of Vocabulary\n",
    "\n",
    "4- Semantic meaning: the basic BOW approach does not consider the meaning of the word in the document. It completely ignores the context in which it’s used. The same word can be used in multiple places based on the context or nearby words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages:\n",
    "\n",
    "1- Very simple to understand and implement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================================================================\n",
    "\n",
    "##### NOTE: More advanced way of representing text data is by embeddings or word vectors. Read the different ways here. And here is how to evaluate word vectors.\n",
    "\n",
    "##### Example:\n",
    "\n",
    "Suppose the vocabulary contains the words : { and, cat, dog, jumped, sat, over, ran, the }. You have the following sentence:  “The fox jumped over the dog and the dog ran”. Bag of words representation for this toy example: [1 0 2 1 0 1 1 3].\n",
    "\n",
    "##### Nonzero values:\n",
    "\n",
    "As the word and occurs only once in the sentence, there is value of 1 for the feature and. The word dog occurs twice in the sentence and hence a value of 2 for the feature dog.\n",
    "\n",
    "###### Zero values:\n",
    "\n",
    "There is no word cat in the sentence and hence a 0 for the feature cat. In a real dataset, the vocabulary contains 50K to 100K words leading to extremely high dimensional sparse vectors. Techniques for dimensionality reduction are typically used to handle bag of words vectors.   \n",
    "\n",
    "##### Where does it fail ?\n",
    "\n",
    "When we want to capture more context (what word appeared  after some other word) and not just co-occurrence in the same document. Sometimes bag of bigrams are used to capture some context, though they are very expensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================\n",
    "\n",
    "#### Popular and simple method of feature extraction with text data which are currently used are:\n",
    "    \n",
    "1- Bag-of-Words\n",
    "\n",
    "2- TF-IDF\n",
    "\n",
    "3- Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## references\n",
    "https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
    "https://medium.com/greyatom/an-introduction-to-bag-of-words-in-nlp-ac967d43b428\n",
    "https://www.machinelearningaptitude.com/topics/natural-language-processing/what-are-some-advantages-and-disadvantages-using-bag-of-words-where-would-you-use-it-and-where-would-you-not/#:~:text=Disadvantages%3A,are%20independent%20of%20each%20other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Step #1 : We will first preprocess the data, in order to:\n",
    "\n",
    "1-Convert text to lower case.\n",
    "\n",
    "2- Remove all non-word characters.\n",
    "\n",
    "3- Remove all punctuations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Beans. I was trying to explain to somebody as we were flying in, that’s corn. That’s beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen in a long time, and somehow he has not aged and I have. And it’s great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 code for preprocessing text \n",
    "import nltk \n",
    "import re \n",
    "import numpy as np \n",
    "  \n",
    "# execute the text here as : \n",
    "# text = \"\"\" # place text here  \"\"\" \n",
    "dataset = nltk.sent_tokenize(text) \n",
    "for i in range(len(dataset)): \n",
    "    dataset[i] = dataset[i].lower() \n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step #2 : Obtaining most frequent words in our text.\n",
    "\n",
    "We will apply the following steps to generate our model.\n",
    "\n",
    "We declare a dictionary to hold our bag of words.\n",
    "Next we tokenize each sentence to words.\n",
    "Now for each word in sentence, we check if the word exists in our dictionary.\n",
    "If it does, then we increment its count by 1. If it doesn’t, we add it to our dictionary and set its count as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model \n",
    "word2count = {} \n",
    "for data in dataset: \n",
    "    words = nltk.word_tokenize(data) \n",
    "    for word in words: \n",
    "        if word not in word2count.keys(): \n",
    "            word2count[word] = 1\n",
    "        else: \n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step #3 : Building the Bag of Words model\n",
    "In this step we construct a vector, which would tell us whether a word in each sentence is a \n",
    "frequent word or not. If a word in a sentence is a frequent word, we set it as 1, else we set it as 0.\n",
    "This can be implemented with the help of following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = [] \n",
    "for data in dataset: \n",
    "    vector = [] \n",
    "    for word in freq_words: \n",
    "        if word in nltk.word_tokenize(data): \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "In our model, we have a total of 118 words. However when processing large texts, the number of words \n",
    "could reach millions. We do not need to use all those words. Hence, we select a particular \n",
    "number of most frequently used words. To implement this we use:\n",
    "    \n",
    "\n",
    "import heapq \n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Another way\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical stuff\n",
    "https://www.geeksforgeeks.org/bag-of-words-bow-model-in-nlp/\n",
    "\n",
    "https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/#:~:text=A%20simple%20and%20effective%20model,each%20word%20a%20unique%20number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
