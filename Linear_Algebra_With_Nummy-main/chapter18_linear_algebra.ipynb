{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An important machine learning method for dimensionality reduction is called Principal Component Analysis. \n",
    "It is a method that uses simple matrix operations from linear algebra and\n",
    "statistics to calculate a projection of the original data into the same number or fewer dimensions.\n",
    "\n",
    "1. What is Principal Component Analysis\n",
    "2. Calculate Principal Component Analysis\n",
    "3. Principal Component Analysis in scikit-learn\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Principal Component Analysis, or PCA for short, is a method for reducing the dimensionality\n",
    "of data\n",
    "\n",
    "PCA is an operation applied to a dataset, represented by an n × m matrix A that results in a\n",
    "projection of A which we will call B. Let’s walk through the steps of this operation.\n",
    "\n",
    "A = (a1,1 a1,2\n",
    "    a2,1 a2,2\n",
    "    a3,1 a3,2)\n",
    "B = P CA(A)\n",
    "\n",
    "The first step is to calculate the mean values of each column.\n",
    "M = mean(A)\n",
    "Next, we need to center the values in each column by subtracting the mean column value.\n",
    "C = A − M\n",
    "\n",
    "The next step is to calculate the covariance matrix of the centered matrix C. Correlation\n",
    "is a normalized measure of the amount and direction (positive or negative) that two columns\n",
    "change together. Covariance is a generalized and unnormalized version of correlation across\n",
    "multiple columns. A covariance matrix is a calculation of covariance of a given matrix with\n",
    "covariance scores for every column with every other column, including itself\n",
    "\n",
    "V = cov(C)\n",
    "\n",
    "Finally, we calculate the eigendecomposition of the covariance matrix V . This results in a\n",
    "list of eigenvalues and a list of eigenvectors.\n",
    "\n",
    "values, vectors = eig(V)\n",
    "\n",
    "The eigenvectors represent the directions or components for the reduced subspace of B,\n",
    "whereas the eigenvalues represent the magnitudes for the directions. The eigenvectors can be\n",
    "sorted by the eigenvalues in descending order to provide a ranking of the components or axes of\n",
    "the new subspace for A. If all eigenvalues have a similar value, then we know that the existing\n",
    "representation may already be reasonably compressed or dense and that the projection may\n",
    "offer little. If there are eigenvalues close to zero, they represent components or axes of B that\n",
    "may be discarded. A total of m or less components must be selected to comprise the chosen\n",
    "subspace. Ideally, we would select k eigenvectors, called principal components, that have the k\n",
    "largest eigenvalues.\n",
    "\n",
    "B = select(values, vectors)\n",
    "\n",
    "Other matrix decomposition methods can be used such as Singular-Value Decomposition,\n",
    "or SVD. As such, generally the values are referred to as singular values and the vectors of the\n",
    "subspace are referred to as principal components. Once chosen, data can be projected into the\n",
    "subspace via matrix multiplication\n",
    "\n",
    "P = BT· A\n",
    "Where T is Superscript of B\n",
    "\n",
    "Where A is the original data that we wish to project, BT is the transpose of the chosen\n",
    "principal components and P is the projection of A. This is called the covariance method for\n",
    "calculating the PCA, although there are alternative ways to calculate it\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "There is no pca() function in NumPy, but we can easily calculate the Principal Component\n",
    "Analysis step-by-step using NumPy functions. The example below defines a small 3 × 2 matrix,\n",
    "centers the data in the matrix, calculates the covariance matrix of the centered data, and then\n",
    "the eigendecomposition of the covariance matrix. The eigenvectors and eigenvalues are taken as\n",
    "the principal components and singular values and used to project the original data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "[8. 0.]\n",
      "[[-2.82842712  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 2.82842712  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# principal component analysis\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import cov\n",
    "from numpy.linalg import eig\n",
    "# define matrix\n",
    "A = array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "print(A)\n",
    "# column means\n",
    "M = mean(A.T, axis=1)\n",
    "# center columns by subtracting column means\n",
    "C = A - M\n",
    "# calculate covariance matrix of centered matrix\n",
    "V = cov(C.T)\n",
    "# factorize covariance matrix\n",
    "values, vectors = eig(V)\n",
    "print(vectors)\n",
    "print(values)\n",
    "# project data\n",
    "P = vectors.T.dot(C.T)\n",
    "print(P.T)\n",
    "\n",
    "\"\"\"\n",
    "Running the example first prints the original matrix, then the eigenvectors and eigenvalues\n",
    "of the centered covariance matrix, followed finally by the projection of the original matrix.\n",
    "Interestingly, we can see that only the first eigenvector is required, suggesting that we could\n",
    "project our 3 × 2 matrix onto a 3 × 1 matrix with little loss.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can calculate a Principal Component Analysis on a dataset using the PCA() class in the\n",
    "scikit-learn library. The benefit of this approach is that once the projection is calculated, it can\n",
    "be applied to new data again and again quite easily. When creating the class, the number of\n",
    "components can be specified as a parameter. The class is first fit on a dataset by calling the fit()\n",
    "function, and then the original dataset or other data can be projected into a subspace with the\n",
    "chosen number of dimensions by calling the transform() function. Once fit, the singular values\n",
    "and principal components can be accessed on the PCA class via the explained variance and\n",
    "components attributes. The example below demonstrates using this class by first creating an\n",
    "instance, fitting it on a 3 × 2 matrix, accessing the values and vectors of the projection, and\n",
    "transforming the original data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[ 0.70710678  0.70710678]\n",
      " [-0.70710678  0.70710678]]\n",
      "[8. 0.]\n",
      "[[-2.82842712e+00 -2.22044605e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 2.82842712e+00  2.22044605e-16]]\n"
     ]
    }
   ],
   "source": [
    "# principal component analysis with scikit-learn\n",
    "from numpy import array\n",
    "from sklearn.decomposition import PCA\n",
    "# define matrix\n",
    "A = array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "print(A)\n",
    "# create the transform\n",
    "pca = PCA(2)\n",
    "# fit transform\n",
    "pca.fit(A)\n",
    "# access values and vectors\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)\n",
    "# transform data\n",
    "B = pca.transform(A)\n",
    "print(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
