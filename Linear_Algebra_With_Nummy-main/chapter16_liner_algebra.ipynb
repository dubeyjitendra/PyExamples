{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Matrix decomposition, also known as matrix factorization, involves describing a given matrix\n",
    "using its constituent elements. Perhaps the most known and widely used matrix decomposition\n",
    "method is the Singular-Value Decomposition, or SVD. All matrices have an SVD, which makes\n",
    "it more stable than other methods, such as the eigendecomposition. As such, it is often used\n",
    "in a wide array of applications including compressing, denoising, and data reduction\n",
    "\n",
    "1. What is the Singular-Value Decomposition\n",
    "2. Calculate Singular-Value Decomposition\n",
    "3. Reconstruct Matrix\n",
    "4. Pseudoinverse\n",
    "5. Dimensionality Reduction\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Singular-Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for\n",
    "reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations\n",
    "simpler. For the case of simplicity we will focus on the SVD for real-valued matrices and ignore\n",
    "the case for complex numbers\"\"\"\n",
    "\n",
    "A = U · Σ · VT(T is superscript)\n",
    "\n",
    "Where A is the real n × m matrix that we wish to decompose, U is an m × m matrix, Σ\n",
    "represented by the uppercase Greek letter sigma) is an m × n diagonal matrix, and VT\n",
    "is the V transpose of an n × n matrix where T is a superscript\n",
    "\n",
    "The singular value decomposition (SVD) provides another way to factorize a matrix,\n",
    "into singular vectors and singular values. The SVD allows us to discover some of\n",
    "the same kind of information as the eigendecomposition. However, the SVD is more\n",
    "generally applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses of SVD\n",
    "The SVD is used widely both in the calculation of other matrix operations, such as matrix\n",
    "inverse, but also as a data reduction method in machine learning. SVD can also be used in least\n",
    "squares linear regression, image compression, and denoising data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Singular-Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[-0.2298477   0.88346102  0.40824829]\n",
      " [-0.52474482  0.24078249 -0.81649658]\n",
      " [-0.81964194 -0.40189603  0.40824829]]\n",
      "[9.52551809 0.51430058]\n",
      "[[-0.61962948 -0.78489445]\n",
      " [-0.78489445  0.61962948]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The SVD can be calculated by calling the svd() function. The function takes a matrix and\n",
    "returns the U, Σ and VT(T is super script)\n",
    "elements. The Σ diagonal matrix is returned as a vector of singular\n",
    "values. The V matrix is returned in a transposed form, e.g. VT(T is super script)\n",
    ". The example below defines a 3 × 2 matrix and calculates the singular-value decomposition.\"\"\"\n",
    "\n",
    "# singular-value decomposition\n",
    "from numpy import array\n",
    "from scipy.linalg import svd\n",
    "# define a matrix\n",
    "A = array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "print(A)\n",
    "# factorize\n",
    "U, s, V = svd(A)\n",
    "print(U)\n",
    "print(s)\n",
    "print(V)\n",
    "\n",
    "\"\"\"Running the example first prints the defined 3×2 matrix, then the 3×3 U matrix, 2 element\n",
    "Σ vector, and 2 × 2 VT(T is superscript) matrix elements calculated from the decomposition\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The original matrix can be reconstructed from the U, Σ, and VT\n",
    "elements. The U, s, and V\n",
    "elements returned from the svd() cannot be multiplied directly. The s vector must be converted\n",
    "into a diagonal matrix using the diag() function. By default, this function will create a square\n",
    "matrix that is m × m, relative to our original matrix. This causes a problem as the size of\n",
    "the matrices do not fit the rules of matrix multiplication, where the number of columns in a\n",
    "matrix must match the number of rows in the subsequent matrix. After creating the square Σ\n",
    "diagonal matrix, the sizes of the matrices are relative to the original n × m matrix that we are\n",
    "decomposing, as follows:\n",
    "U(m × m) · Σ(m × m) · VT(T is superscript)(n × n)\"\"\"\n",
    "\n",
    "U(m × m) · Σ(m × n) · VT(n × n)\n",
    "\n",
    "Where T is super script of V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "## Sample output from reconstructing a rectangular matrix from a SVD.\n",
    "# reconstruct rectangular matrix from svd\n",
    "from numpy import array\n",
    "from numpy import diag\n",
    "from numpy import zeros\n",
    "from scipy.linalg import svd\n",
    "# define matrix\n",
    "A = array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "print(A)\n",
    "# factorize\n",
    "U, s, V = svd(A)\n",
    "# create m x n Sigma matrix\n",
    "Sigma = zeros((A.shape[0], A.shape[1]))\n",
    "# populate Sigma with n x n diagonal matrix\n",
    "Sigma[:A.shape[1], :A.shape[1]] = diag(s)\n",
    "# reconstruct matrix\n",
    "B = U.dot(Sigma.dot(V))\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The above complication with the Σ diagonal only exists with the case where m and n are\n",
    "not equal. The diagonal matrix can be used directly when reconstructing a square matrix, as\n",
    "follows.\"\"\"\n",
    "\n",
    "# reconstruct square matrix from svd\n",
    "from numpy import array\n",
    "from numpy import diag\n",
    "from scipy.linalg import svd\n",
    "# define matrix\n",
    "A = array([\n",
    "[1, 2, 3],\n",
    "[4, 5, 6],\n",
    "[7, 8, 9]])\n",
    "print(A)\n",
    "# factorize\n",
    "U, s, V = svd(A)\n",
    "# create n x n Sigma matrix\n",
    "Sigma = diag(s)\n",
    "# reconstruct matrix\n",
    "B = U.dot(Sigma.dot(V))\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudoinverse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The pseudoinverse is the generalization of the matrix inverse for square matrices to rectangular\n",
    "matrices where the number of rows and columns are not equal. It is also called the Moore-Penrose\n",
    "Inverse after two independent discoverers of the method or the Generalized Inverse.\"\"\"\n",
    "\n",
    "\"\"\"Matrix inversion is not defined for matrices that are not square. [...] When A has\n",
    "more columns than rows, then solving a linear equation using the pseudoinverse\n",
    "provides one of the many possible solutions.\n",
    "— Page 46, Deep Learning, 2016.\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"The pseudoinverse is denoted as A+, where A is the matrix that is being inverted and + is a\n",
    "superscript. The pseudoinverse is calculated using the singular value decomposition of A:\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"A+ = V · D+ · UT\"\"\"\n",
    "where +, T is super script\n",
    "\n",
    "\"\"\"A = U · Σ · VT\"\"\"\n",
    "\n",
    "where T is super script\n",
    "\n",
    "\"\"\"The D+ can be calculated by creating a diagonal matrix from Σ, calculating the reciprocal\n",
    "of each non-zero element in Σ, and taking the transpose if the original matrix was rectangular.\"\"\"\n",
    "\n",
    "\"\"\"Where + is super sccript\"\"\"\n",
    "\n",
    "Σ =(s1,1 0 0\n",
    "    0 s2,2 0\n",
    "    0 0 s3,3)\n",
    "\n",
    "\n",
    "D+ =  (1/s1,1  0        0\n",
    "      0        1 1/s2,  2\n",
    "      0        0        1/s3,3)\n",
    "\n",
    "\n",
    "\"\"\"The pseudoinverse provides one way of solving the linear regression equation, specifically\n",
    "when there are more rows than there are columns, which is often the case. NumPy provides the\n",
    "function pinv() for calculating the pseudoinverse of a rectangular matrix. The example below\n",
    "defines a 4 × 2 matrix and calculates the pseudoinverse.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.2]\n",
      " [0.3 0.4]\n",
      " [0.5 0.6]\n",
      " [0.7 0.8]]\n",
      "[[-1.00000000e+01 -5.00000000e+00  1.28785871e-14  5.00000000e+00]\n",
      " [ 8.50000000e+00  4.50000000e+00  5.00000000e-01 -3.50000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# pseudoinverse\n",
    "from numpy import array\n",
    "from numpy.linalg import pinv\n",
    "# define matrix\n",
    "A = array([\n",
    "[0.1, 0.2],\n",
    "[0.3, 0.4],\n",
    "[0.5, 0.6],\n",
    "[0.7, 0.8]])\n",
    "print(A)\n",
    "# calculate pseudoinverse\n",
    "B = pinv(A)\n",
    "print(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We can calculate the pseudoinverse manually via the SVD and compare the results to the\n",
    "pinv() function. First we must calculate the SVD. Next we must calculate the reciprocal of\n",
    "each value in the s array. Then the s array can be transformed into a diagonal matrix with an\n",
    "added row of zeros to make it rectangular. Finally, we can calculate the pseudoinverse from the\n",
    "elements. The specific implementation is:\n",
    "\n",
    "A+ = VT· DT· UT\n",
    "\n",
    "where +, T is super script\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.2]\n",
      " [0.3 0.4]\n",
      " [0.5 0.6]\n",
      " [0.7 0.8]]\n",
      "[[-1.00000000e+01 -5.00000000e+00  1.28508315e-14  5.00000000e+00]\n",
      " [ 8.50000000e+00  4.50000000e+00  5.00000000e-01 -3.50000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# pseudoinverse via svd\n",
    "from numpy import array\n",
    "from numpy.linalg import svd\n",
    "from numpy import zeros\n",
    "from numpy import diag\n",
    "# define matrix\n",
    "A = array([\n",
    "[0.1, 0.2],\n",
    "[0.3, 0.4],\n",
    "[0.5, 0.6],\n",
    "[0.7, 0.8]])\n",
    "print(A)\n",
    "# factorize\n",
    "U, s, V = svd(A)\n",
    "# reciprocals of s\n",
    "d = 1.0 / s\n",
    "# create m x n D matrix\n",
    "D = zeros(A.shape)\n",
    "# populate D with n x n diagonal matrix\n",
    "D[:A.shape[1], :A.shape[1]] = diag(d)\n",
    "# calculate pseudoinverse\n",
    "B = V.T.dot(D.T).dot(U.T)\n",
    "print(B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A popular application of SVD is for dimensionality reduction. Data with a large number of\n",
    "features, such as more features (columns) than observations (rows) may be reduced to a smaller\n",
    "subset of features that are most relevant to the prediction problem.\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"The result is a matrix with\n",
    "a lower rank that is said to approximate the original matrix. To do this we can perform an SVD\n",
    "operation on the original data and select the top k largest singular values in Σ. These columns\n",
    "can be selected from Σ and the rows selected from VT\n",
    ". An approximate B of the original vector\n",
    "A can then be reconstructed\"\"\"\n",
    "\n",
    "\"\"\"B = U · Σk · VT\n",
    "\n",
    "Where k is lower script and T is super script\n",
    "\n",
    "In natural language processing, this approach can be used on matrices of word occurrences\n",
    "or word frequencies in documents and is called Latent Semantic Analysis or Latent Semantic\n",
    "Indexing\n",
    "\n",
    "In practice, we can retain and work with a descriptive subset of the data called T.\n",
    "This is a dense summary of the matrix or a projection\n",
    "\n",
    "T = U · Σk\n",
    "where k is lower script\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"The example below demonstrates data reduction with the SVD. First a 3 × 10 matrix is\n",
    "defined, with more columns than rows. The SVD is calculated and only the first two features\n",
    "are selected. The elements are recombined to give an accurate reproduction of the original\n",
    "matrix. Finally the transform is calculated two different ways\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6  7  8  9 10]\n",
      " [11 12 13 14 15 16 17 18 19 20]\n",
      " [21 22 23 24 25 26 27 28 29 30]]\n",
      "[[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      " [11. 12. 13. 14. 15. 16. 17. 18. 19. 20.]\n",
      " [21. 22. 23. 24. 25. 26. 27. 28. 29. 30.]]\n",
      "[[-18.52157747   6.47697214]\n",
      " [-49.81310011   1.91182038]\n",
      " [-81.10462276  -2.65333138]]\n",
      "[[-18.52157747   6.47697214]\n",
      " [-49.81310011   1.91182038]\n",
      " [-81.10462276  -2.65333138]]\n"
     ]
    }
   ],
   "source": [
    "# data reduction with svd\n",
    "from numpy import array\n",
    "from numpy import diag\n",
    "from numpy import zeros\n",
    "from scipy.linalg import svd\n",
    "# define matrix\n",
    "A = array([\n",
    "[1,2,3,4,5,6,7,8,9,10],\n",
    "[11,12,13,14,15,16,17,18,19,20],\n",
    "[21,22,23,24,25,26,27,28,29,30]])\n",
    "print(A)\n",
    "# factorize\n",
    "U, s, V = svd(A)\n",
    "# create m x n Sigma matrix\n",
    "Sigma = zeros((A.shape[0], A.shape[1]))\n",
    "# populate Sigma with n x n diagonal matrix\n",
    "Sigma[:A.shape[0], :A.shape[0]] = diag(s)\n",
    "# select\n",
    "n_elements = 2\n",
    "Sigma = Sigma[:, :n_elements]\n",
    "V = V[:n_elements, :]\n",
    "# reconstruct\n",
    "B = U.dot(Sigma.dot(V))\n",
    "print(B)\n",
    "# transform\n",
    "T = U.dot(Sigma)\n",
    "print(T)\n",
    "T = A.dot(V.T)\n",
    "print(T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6  7  8  9 10]\n",
      " [11 12 13 14 15 16 17 18 19 20]\n",
      " [21 22 23 24 25 26 27 28 29 30]]\n",
      "[[18.52157747  6.47697214]\n",
      " [49.81310011  1.91182038]\n",
      " [81.10462276 -2.65333138]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The scikit-learn provides a TruncatedSVD class that implements this capability directly. The\n",
    "TruncatedSVD class can be created in which you must specify the number of desirable features\n",
    "or components to select, e.g. 2. Once created, you can fit the transform (e.g. calculate VTk)\n",
    "by calling the fit() function, then apply it to the original matrix by calling the transform()\n",
    "function. The result is the transform of A called T above. The example below demonstrates the\n",
    "TruncatedSVD class.\n",
    "\"\"\"\n",
    "\n",
    "### Sample output from calculating data reduction with the SVD in scikit-learn.\n",
    "# svd data reduction in scikit-learn\n",
    "from numpy import array\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# define matrix\n",
    "A = array([\n",
    "[1,2,3,4,5,6,7,8,9,10],\n",
    "[11,12,13,14,15,16,17,18,19,20],\n",
    "[21,22,23,24,25,26,27,28,29,30]])\n",
    "print(A)\n",
    "# create transform\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "# fit transform\n",
    "svd.fit(A)\n",
    "# apply transform\n",
    "result = svd.transform(A)\n",
    "print(result)\n",
    "\"\"\"Running the example first prints the defined matrix, followed by the transformed version\n",
    "of the matrix. We can see that the values match those calculated manually above, except for\n",
    "the sign on some values. We can expect there to be some instability when it comes to the\n",
    "sign given the nature of the calculations involved and the differences in the underlying libraries\n",
    "and methods used. This instability of sign should not be a problem in practice as long as the\n",
    "transform is trained for reuse.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
